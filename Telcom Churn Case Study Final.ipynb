
Problem Statement
In the telecom industry, customers are able to choose from multiple service providers and actively switch from one operator to another. In this highly competitive market, the telecommunications industry experiences an average of 15-25% annual churn rate. Given the fact that it costs 5-10 times more to acquire a new customer than to retain an existing one, customer retention has now become even more important than customer acquisition.

For many incumbent operators, retaining highly profitable customers is the number one business goal.

To reduce customer churn, telecom companies need to predict which customers are at high risk of churn.

In this project, you will analyze customer-level data of a leading telecom firm, build predictive models to identify customers at high risk of churn and identify the main indicators of churn.

Steps to be followed
Data Understanding
Data Cleaning
Filtering High-Value Customers
Creating target Variable
Deriving New Features
Handling Missing values
Data Visualization-Univariate Analysis
Data Visualization- Bivariate Analysis
Outlier Detection
Data Preparation
Data Modeling and Evaluation
Non-Interpretable Models
Interpretable Models
Conclusion
1. Data Understanding
[ ]
  1

%%HTML
<style type="text/css">
table.dataframe td, table.dataframe th {
    border: 1px  black solid !important;
  color: black !important;
}
</style>

[ ]
  1
  2
  3
  4
  5
  6
  7
  8
  9
 10
 11
 12
 13
 14
 15
 16
 17
 18
 19
 20
 21
 22
 23
 24
 25
 26
 27
 28
 29
 30
 31
 32
 33
 34
 35
 36
 37
 38
 39
 40
 41
 42
 43
 44
 45
 46
 47
 48
 49
 50
 51
 52
 53
 54
 55
 56
 57
 58
 59
 60
 61
 62
 63
 64
 65
 66
 67
 68
 69
 70
#Importing Data Reading and Processing Libraries
import pandas as pd
import numpy as np

#Imporitng Data Visualization Libraries
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns

#Importing Data Preparation and Modeling Libraries
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import KFold, GridSearchCV,StratifiedKFold
from sklearn.metrics import accuracy_score, recall_score, classification_report,confusion_matrix,roc_auc_score,roc_curve
from sklearn.decomposition import PCA

from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier

#Importing Warning Libraries
import warnings
warnings.filterwarnings("ignore")

#Importing Miscellaneous Libraries
pd.set_option("display.max_columns",None)
pd.set_option("display.max_rows",None)
pd.set_option('display.width', None)

# Import the StandardScaler()
from sklearn.preprocessing import StandardScaler

#Improting the PCA module
from sklearn.decomposition import PCA

# For Hopkins test
from sklearn.neighbors import NearestNeighbors
from random import sample
from numpy.random import uniform
import numpy as np
from math import isnan

# For clustering 
## using KMeans ##
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

# Importing classification report and confusion matrix from sklearn metrics
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

## using Hierarchical ##
from scipy.cluster.hierarchy import linkage
from scipy.cluster.hierarchy import dendrogram
from scipy.cluster.hierarchy import cut_tree

# Importing required packages for visualization
from IPython.display import Image  
from sklearn.externals.six import StringIO  
from sklearn.tree import export_graphviz
import pydot, graphviz
from sklearn.svm import SVC

# Other sklearn packages
import sklearn.metrics as metrics
from xgboost import XGBClassifier
from sklearn.tree import DecisionTreeClassifier

from datetime import date,datetime
import math
import multiprocessing
[ ]
  1
  2
  3
  4
#Loading the Dataset

df = pd.read_csv('telecom_churn_data.csv')
df.head()

[ ]
  1
df.shape
(99999, 226)
[ ]
  1
df.info(verbose=1)
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 99999 entries, 0 to 99998
Data columns (total 226 columns):
 #   Column                    Dtype  
---  ------                    -----  
 0   mobile_number             int64  
 1   circle_id                 int64  
 2   loc_og_t2o_mou            float64
 3   std_og_t2o_mou            float64
 4   loc_ic_t2o_mou            float64
 5   last_date_of_month_6      object 
 6   last_date_of_month_7      object 
 7   last_date_of_month_8      object 
 8   last_date_of_month_9      object 
 9   arpu_6                    float64
 10  arpu_7                    float64
 11  arpu_8                    float64
 12  arpu_9                    float64
 13  onnet_mou_6               float64
 14  onnet_mou_7               float64
 15  onnet_mou_8               float64
 16  onnet_mou_9               float64
 17  offnet_mou_6              float64
 18  offnet_mou_7              float64
 19  offnet_mou_8              float64
 20  offnet_mou_9              float64
 21  roam_ic_mou_6             float64
 22  roam_ic_mou_7             float64
 23  roam_ic_mou_8             float64
 24  roam_ic_mou_9             float64
 25  roam_og_mou_6             float64
 26  roam_og_mou_7             float64
 27  roam_og_mou_8             float64
 28  roam_og_mou_9             float64
 29  loc_og_t2t_mou_6          float64
 30  loc_og_t2t_mou_7          float64
 31  loc_og_t2t_mou_8          float64
 32  loc_og_t2t_mou_9          float64
 33  loc_og_t2m_mou_6          float64
 34  loc_og_t2m_mou_7          float64
 35  loc_og_t2m_mou_8          float64
 36  loc_og_t2m_mou_9          float64
 37  loc_og_t2f_mou_6          float64
 38  loc_og_t2f_mou_7          float64
 39  loc_og_t2f_mou_8          float64
 40  loc_og_t2f_mou_9          float64
 41  loc_og_t2c_mou_6          float64
 42  loc_og_t2c_mou_7          float64
 43  loc_og_t2c_mou_8          float64
 44  loc_og_t2c_mou_9          float64
 45  loc_og_mou_6              float64
 46  loc_og_mou_7              float64
 47  loc_og_mou_8              float64
 48  loc_og_mou_9              float64
 49  std_og_t2t_mou_6          float64
 50  std_og_t2t_mou_7          float64
 51  std_og_t2t_mou_8          float64
 52  std_og_t2t_mou_9          float64
 53  std_og_t2m_mou_6          float64
 54  std_og_t2m_mou_7          float64
 55  std_og_t2m_mou_8          float64
 56  std_og_t2m_mou_9          float64
 57  std_og_t2f_mou_6          float64
 58  std_og_t2f_mou_7          float64
 59  std_og_t2f_mou_8          float64
 60  std_og_t2f_mou_9          float64
 61  std_og_t2c_mou_6          float64
 62  std_og_t2c_mou_7          float64
 63  std_og_t2c_mou_8          float64
 64  std_og_t2c_mou_9          float64
 65  std_og_mou_6              float64
 66  std_og_mou_7              float64
 67  std_og_mou_8              float64
 68  std_og_mou_9              float64
 69  isd_og_mou_6              float64
 70  isd_og_mou_7              float64
 71  isd_og_mou_8              float64
 72  isd_og_mou_9              float64
 73  spl_og_mou_6              float64
 74  spl_og_mou_7              float64
 75  spl_og_mou_8              float64
 76  spl_og_mou_9              float64
 77  og_others_6               float64
 78  og_others_7               float64
 79  og_others_8               float64
 80  og_others_9               float64
 81  total_og_mou_6            float64
 82  total_og_mou_7            float64
 83  total_og_mou_8            float64
 84  total_og_mou_9            float64
 85  loc_ic_t2t_mou_6          float64
 86  loc_ic_t2t_mou_7          float64
 87  loc_ic_t2t_mou_8          float64
 88  loc_ic_t2t_mou_9          float64
 89  loc_ic_t2m_mou_6          float64
 90  loc_ic_t2m_mou_7          float64
 91  loc_ic_t2m_mou_8          float64
 92  loc_ic_t2m_mou_9          float64
 93  loc_ic_t2f_mou_6          float64
 94  loc_ic_t2f_mou_7          float64
 95  loc_ic_t2f_mou_8          float64
 96  loc_ic_t2f_mou_9          float64
 97  loc_ic_mou_6              float64
 98  loc_ic_mou_7              float64
 99  loc_ic_mou_8              float64
 100 loc_ic_mou_9              float64
 101 std_ic_t2t_mou_6          float64
 102 std_ic_t2t_mou_7          float64
 103 std_ic_t2t_mou_8          float64
 104 std_ic_t2t_mou_9          float64
 105 std_ic_t2m_mou_6          float64
 106 std_ic_t2m_mou_7          float64
 107 std_ic_t2m_mou_8          float64
 108 std_ic_t2m_mou_9          float64
 109 std_ic_t2f_mou_6          float64
 110 std_ic_t2f_mou_7          float64
 111 std_ic_t2f_mou_8          float64
 112 std_ic_t2f_mou_9          float64
 113 std_ic_t2o_mou_6          float64
 114 std_ic_t2o_mou_7          float64
 115 std_ic_t2o_mou_8          float64
 116 std_ic_t2o_mou_9          float64
 117 std_ic_mou_6              float64
 118 std_ic_mou_7              float64
 119 std_ic_mou_8              float64
 120 std_ic_mou_9              float64
 121 total_ic_mou_6            float64
 122 total_ic_mou_7            float64
 123 total_ic_mou_8            float64
 124 total_ic_mou_9            float64
 125 spl_ic_mou_6              float64
 126 spl_ic_mou_7              float64
 127 spl_ic_mou_8              float64
 128 spl_ic_mou_9              float64
 129 isd_ic_mou_6              float64
 130 isd_ic_mou_7              float64
 131 isd_ic_mou_8              float64
 132 isd_ic_mou_9              float64
 133 ic_others_6               float64
 134 ic_others_7               float64
 135 ic_others_8               float64
 136 ic_others_9               float64
 137 total_rech_num_6          int64  
 138 total_rech_num_7          int64  
 139 total_rech_num_8          int64  
 140 total_rech_num_9          int64  
 141 total_rech_amt_6          int64  
 142 total_rech_amt_7          int64  
 143 total_rech_amt_8          int64  
 144 total_rech_amt_9          int64  
 145 max_rech_amt_6            int64  
 146 max_rech_amt_7            int64  
 147 max_rech_amt_8            int64  
 148 max_rech_amt_9            int64  
 149 date_of_last_rech_6       object 
 150 date_of_last_rech_7       object 
 151 date_of_last_rech_8       object 
 152 date_of_last_rech_9       object 
 153 last_day_rch_amt_6        int64  
 154 last_day_rch_amt_7        int64  
 155 last_day_rch_amt_8        int64  
 156 last_day_rch_amt_9        int64  
 157 date_of_last_rech_data_6  object 
 158 date_of_last_rech_data_7  object 
 159 date_of_last_rech_data_8  object 
 160 date_of_last_rech_data_9  object 
 161 total_rech_data_6         float64
 162 total_rech_data_7         float64
 163 total_rech_data_8         float64
 164 total_rech_data_9         float64
 165 max_rech_data_6           float64
 166 max_rech_data_7           float64
 167 max_rech_data_8           float64
 168 max_rech_data_9           float64
 169 count_rech_2g_6           float64
 170 count_rech_2g_7           float64
 171 count_rech_2g_8           float64
 172 count_rech_2g_9           float64
 173 count_rech_3g_6           float64
 174 count_rech_3g_7           float64
 175 count_rech_3g_8           float64
 176 count_rech_3g_9           float64
 177 av_rech_amt_data_6        float64
 178 av_rech_amt_data_7        float64
 179 av_rech_amt_data_8        float64
 180 av_rech_amt_data_9        float64
 181 vol_2g_mb_6               float64
 182 vol_2g_mb_7               float64
 183 vol_2g_mb_8               float64
 184 vol_2g_mb_9               float64
 185 vol_3g_mb_6               float64
 186 vol_3g_mb_7               float64
 187 vol_3g_mb_8               float64
 188 vol_3g_mb_9               float64
 189 arpu_3g_6                 float64
 190 arpu_3g_7                 float64
 191 arpu_3g_8                 float64
 192 arpu_3g_9                 float64
 193 arpu_2g_6                 float64
 194 arpu_2g_7                 float64
 195 arpu_2g_8                 float64
 196 arpu_2g_9                 float64
 197 night_pck_user_6          float64
 198 night_pck_user_7          float64
 199 night_pck_user_8          float64
 200 night_pck_user_9          float64
 201 monthly_2g_6              int64  
 202 monthly_2g_7              int64  
 203 monthly_2g_8              int64  
 204 monthly_2g_9              int64  
 205 sachet_2g_6               int64  
 206 sachet_2g_7               int64  
 207 sachet_2g_8               int64  
 208 sachet_2g_9               int64  
 209 monthly_3g_6              int64  
 210 monthly_3g_7              int64  
 211 monthly_3g_8              int64  
 212 monthly_3g_9              int64  
 213 sachet_3g_6               int64  
 214 sachet_3g_7               int64  
 215 sachet_3g_8               int64  
 216 sachet_3g_9               int64  
 217 fb_user_6                 float64
 218 fb_user_7                 float64
 219 fb_user_8                 float64
 220 fb_user_9                 float64
 221 aon                       int64  
 222 aug_vbc_3g                float64
 223 jul_vbc_3g                float64
 224 jun_vbc_3g                float64
 225 sep_vbc_3g                float64
dtypes: float64(179), int64(35), object(12)
memory usage: 172.4+ MB
[ ]
  1
  2
#Lets check how the data is spread
df.describe()

2.Data cleaning
[ ]
  1
  2
  3
  4
# There are some columns representing volume based users have month specified in their name, while all other columns have used numbers `6,7,8,9` indicating month name
month = ['aug_vbc_3g','jul_vbc_3g','jun_vbc_3g','sep_vbc_3g']
df = df.rename(columns = {'aug_vbc_3g':'3g_vbc_8','jul_vbc_3g':'3g_vbc_7','jun_vbc_3g':'3g_vbc_6','sep_vbc_3g':'3g_vbc_9'})

[ ]
  1
  2
  3
  4
  5
  6
  7
# Converting the date columns to date time format

date_col= [col for col in df.columns if 'date' in col]

for i in df[date_col]:
    df[i] = pd.to_datetime(df[i])

Columns that have just one unique value for all the customers do not really imply anything. Hence removing columns with zero variance from our dataset.

[ ]
  1
  2
  3
  4
  5
  6
  7
# Dropping columns that have only one unique values for all the leads.
cols = []
for i in df.columns:
    if df[i].nunique() ==1:
        cols.append(i)
df = df.drop(cols,1)
df.head()

[ ]
  1
  2
# Let us also drop mobile number column
df = df.drop('mobile_number',1)
[ ]
  1
  2
  3
  4
  5
# Create an id column for identification of each customer
df = df.reset_index()
df = df.rename(columns = {'index':'cust_id'})
df['cust_id'] = df['cust_id']+1
df.head()

[ ]
  1
df.shape
(99999, 210)
Let us check if there are any categorical columns in our dataset. We can check that by finding if any columns have just 2 unique values 1 & 0. These values analogous to yes & no respectively.

[ ]
  1
  2
  3
  4
  5
  6
  7
# Let us create a seperate category for categorical columns
cat_cols = []

for i in df.columns:
    if df[i].nunique()==2:
        cat_cols.append(i)
cat_cols 
['night_pck_user_6',
 'night_pck_user_7',
 'night_pck_user_8',
 'night_pck_user_9',
 'fb_user_6',
 'fb_user_7',
 'fb_user_8',
 'fb_user_9']
Some important insights
Average revenue of call per user
We observe that the minimum values in arpu_6, arpu_7, arpu_8 & arpu_9 are negative. This indicates that some customers are making loss to the company. We will retain them in our study as our criteria of high value customer is based on usage base churn and not on revenue based churn. Removing them might lead to loss of some meaningful insights. Let us observe their importance in the exploratory data aanalysis part and then take a call.

Sachet recharge
sachet recharge are Service schemes with validity smaller than a month. This means that the days of recahrge in sachet recharge should be less than 30. Any service schemes beyond 29 days should mean that the client has done a monyhly recharge or the entry is wrong. Let us cap the values beyond 29 days by the highest number of days recharge below 30 days.
[ ]
12345
df['sachet_2g_6'] = df['sachet_2g_6'].clip(0,28)
df['sachet_2g_7'] = df['sachet_2g_7'].clip(0,29)
df['sachet_2g_8'] = df['sachet_2g_8'].clip(0,29)
df['sachet_3g_7'] = df['sachet_3g_7'].clip(0,24)
df['sachet_3g_8'] = df['sachet_3g_8'].clip(0,29)
[ ]
1
df.shape
(99999, 210)
3.Filtering High Value customers
Those who have recharged with an amount more than or equal to X, where X is greater than 70th percentile of the average recharge amount in the first two months (the good phase)

[ ]
123456
#Derive Total data rech columns describing data rechatrge amount

df['total_data_rech_6'] = df['av_rech_amt_data_6']* df['total_rech_data_6']
df['total_data_rech_7'] = df['av_rech_amt_data_7']* df['total_rech_data_7']
df['total_data_rech_8'] = df['av_rech_amt_data_8']* df['total_rech_data_8']
df['total_data_rech_9'] = df['av_rech_amt_data_9']* df['total_rech_data_9']
Total recharge amount spent in the 6th & 7th month(Good Phase) and the 8th & 9th month

[ ]
  1
  2
  3
  4
df['total_amt_6'] = df[['total_rech_amt_6', 'total_data_rech_6']].sum(axis=1)
df['total_amt_7'] = df[['total_rech_amt_7', 'total_data_rech_7']].sum(axis=1)
df['total_amt_8'] = df[['total_rech_amt_8', 'total_data_rech_8']].sum(axis=1)
df['total_amt_9'] = df[['total_rech_amt_9', 'total_data_rech_9']].sum(axis=1)
Defining total average recharge amount for the good phase for months 6 and 7 (the good phase)

[ ]
  1
  2
df['rech_amt_good_yr'] = df[['total_amt_6', 'total_amt_7']].sum(axis=1)
df['avg_rech_amt_good_yr'] = round(df['rech_amt_good_yr']/2,2)
[ ]
  1
df.head()

[ ]
  1
  2
perc = df['avg_rech_amt_good_yr'].dropna().quantile(0.7)
perc
478.0
[ ]
  1
  2
  3
# Filtering high value customers
high_val = df.loc[df['avg_rech_amt_good_yr'] >= perc]
high_val.head()

[ ]
  1
  2
  3
  4
  5
  6
  7
## We can drop the total_rech_data &  av_rech_amt_data columns for all the months as we have derived new columns using these columns
del_col = ["total_rech_data_6", "total_rech_data_7", "total_rech_data_8", "av_rech_amt_data_6", "av_rech_amt_data_7", "av_rech_amt_data_8"]

high_val = high_val.drop(del_col,1)

## We can also drop the the average recharge amount of the good phase
high_val = high_val.drop('avg_rech_amt_good_yr',1) 
[ ]
  1
high_val.shape
(30001, 213)
4.Creating Target Variable
Tag churners and remove attributes of the churn phase
Tagging the churned customers (churn=1, else 0) based on the fourth month as follows:
Those who have not made any calls (either incoming or outgoing) AND have not used mobile internet even once in the churn phase. The attributes we need to use to tag churners are:

total_ic_mou_9
total_og_mou_9
vol_2g_mb_9
vol_3g_mb_9
[ ]
  1
  2
  3
  4
  5
# let us filter churned customers based on the fourth month

high_val['churn'] = high_val.apply(lambda x: 1 if (x.total_ic_mou_9 == 0 and x.total_og_mou_9 == 0 and x.vol_2g_mb_9 ==0 and x.vol_3g_mb_9==0) else 0, axis=1)
high_val['churn'] = high_val['churn'].astype("str")
high_val.shape
(30001, 214)
[ ]
  1
  2
# Let us see teh number of churned customers
high_val['churn'].value_counts()
0    27560
1     2441
Name: churn, dtype: int64
[ ]
  1
  2
# let us check what's the % of churned customers
print("The Percentage of churned customers is:" , round(100*(high_val.churn.astype("int").sum()/len(high_val)),2))
The Percentage of churned customers is: 8.14
After tagging churners, let us remove all the attributes corresponding to the churn phase (all attributes having ‘ _9’, etc. in their names)

[ ]
  1
  2
  3
col_9 = [i for i in high_val.columns if '9' in i]
high_val = high_val.drop(col_9,1)
high_val.shape
(30001, 160)
[ ]
  1
  2
  3
  4
# let us update our categorical column list

cat_cols = [ele for ele in cat_cols if ele not in col_9]

5.Deriving new features
[ ]
  1
  2
  3
#Conevrt AON in Months
high_val['aon_yr'] = round((high_val['aon']/365),1)

[ ]
  1
  2
high_val.drop('aon', axis=1, inplace=True)
high_val.head()

Let us create bins for our age of network column that explains the years since when a particular client is using the T network.

[ ]
  1
  2
  3
  4
  5
age_range = [ 0,  2,  4,  6,  8, 10, 12]
age_bin = [ 1, 2, 3, 4, 5, 6]
high_val['age_group'] = pd.cut(high_val['aon_yr'], age_range, labels=age_bin)
high_val['age_group'] = high_val['age_group'].astype(str)
high_val['age_group'].head()
0     2
7     2
8     1
21    1
23    1
Name: age_group, dtype: object
[ ]
  1
  2
  3
  4
# let us update our categorical column list

cat_cols.append('age_group')
cat_cols
['night_pck_user_6',
 'night_pck_user_7',
 'night_pck_user_8',
 'fb_user_6',
 'fb_user_7',
 'fb_user_8',
 'age_group']
6.Handling Missing values
a) Checking missing values
[ ]
  1
  2
  3
  4
# Now Checking Null values
null = round(100*(high_val.isnull().sum()/len(high_val.index)),2).sort_values(ascending = False)
null = null[null!=0]
null
fb_user_8                   46.83
count_rech_2g_8             46.83
date_of_last_rech_data_8    46.83
count_rech_3g_8             46.83
max_rech_data_8             46.83
total_data_rech_8           46.83
night_pck_user_8            46.83
arpu_3g_8                   46.83
arpu_2g_8                   46.83
count_rech_3g_6             44.15
max_rech_data_6             44.15
night_pck_user_6            44.15
fb_user_6                   44.15
date_of_last_rech_data_6    44.15
arpu_2g_6                   44.15
total_data_rech_6           44.15
arpu_3g_6                   44.15
count_rech_2g_6             44.15
date_of_last_rech_data_7    43.15
night_pck_user_7            43.15
arpu_3g_7                   43.15
max_rech_data_7             43.15
count_rech_2g_7             43.15
arpu_2g_7                   43.15
count_rech_3g_7             43.15
fb_user_7                   43.15
total_data_rech_7           43.15
loc_og_t2c_mou_8             3.91
spl_ic_mou_8                 3.91
std_ic_t2m_mou_8             3.91
std_ic_t2f_mou_8             3.91
std_ic_mou_8                 3.91
loc_ic_mou_8                 3.91
std_og_t2f_mou_8             3.91
isd_ic_mou_8                 3.91
loc_og_mou_8                 3.91
loc_ic_t2f_mou_8             3.91
ic_others_8                  3.91
std_og_t2m_mou_8             3.91
std_og_t2t_mou_8             3.91
loc_ic_t2m_mou_8             3.91
isd_og_mou_8                 3.91
std_ic_t2t_mou_8             3.91
loc_ic_t2t_mou_8             3.91
loc_og_t2t_mou_8             3.91
og_others_8                  3.91
roam_ic_mou_8                3.91
onnet_mou_8                  3.91
std_og_mou_8                 3.91
roam_og_mou_8                3.91
offnet_mou_8                 3.91
spl_og_mou_8                 3.91
loc_og_t2m_mou_8             3.91
loc_og_t2f_mou_8             3.91
date_of_last_rech_8          1.94
std_og_t2f_mou_6             1.82
std_og_mou_6                 1.82
spl_ic_mou_6                 1.82
og_others_6                  1.82
onnet_mou_6                  1.82
isd_ic_mou_6                 1.82
loc_ic_mou_6                 1.82
offnet_mou_6                 1.82
std_ic_mou_6                 1.82
ic_others_6                  1.82
std_ic_t2f_mou_6             1.82
std_ic_t2t_mou_6             1.82
loc_ic_t2f_mou_6             1.82
roam_ic_mou_6                1.82
isd_og_mou_6                 1.82
std_ic_t2m_mou_6             1.82
loc_og_t2f_mou_6             1.82
loc_ic_t2t_mou_6             1.82
loc_og_t2m_mou_6             1.82
loc_og_t2t_mou_6             1.82
loc_og_mou_6                 1.82
spl_og_mou_6                 1.82
loc_ic_t2m_mou_6             1.82
roam_og_mou_6                1.82
std_og_t2t_mou_6             1.82
std_og_t2m_mou_6             1.82
loc_og_t2c_mou_6             1.82
loc_ic_mou_7                 1.79
loc_ic_t2f_mou_7             1.79
loc_ic_t2m_mou_7             1.79
isd_og_mou_7                 1.79
loc_ic_t2t_mou_7             1.79
std_ic_t2t_mou_7             1.79
spl_og_mou_7                 1.79
og_others_7                  1.79
loc_og_mou_7                 1.79
std_ic_t2m_mou_7             1.79
std_ic_t2f_mou_7             1.79
onnet_mou_7                  1.79
offnet_mou_7                 1.79
roam_ic_mou_7                1.79
roam_og_mou_7                1.79
loc_og_t2t_mou_7             1.79
loc_og_t2m_mou_7             1.79
loc_og_t2f_mou_7             1.79
loc_og_t2c_mou_7             1.79
std_og_t2t_mou_7             1.79
std_og_t2m_mou_7             1.79
std_og_t2f_mou_7             1.79
ic_others_7                  1.79
isd_ic_mou_7                 1.79
spl_ic_mou_7                 1.79
std_og_mou_7                 1.79
std_ic_mou_7                 1.79
date_of_last_rech_7          0.33
date_of_last_rech_6          0.24
dtype: float64
b) Imputation of missing values
Recharge columns in Good and Action Phase
[ ]
  1
  2
  3
  4
  5
# Let us observe missing values in recharge columns 
rech_col = [i for i in high_val.columns if 'rech' in i]
rech_6_col = [i for i in rech_col if '6' in i]
rech_7_col = [i for i in rech_col if '7' in i]
rech_8_col = [i for i in rech_col if '8' in i]
Recharge columns for month- 6(June)

[ ]
  1
  2
  3
  4
  5
  6
  7
  8
# Let us observe missing values in recharge columns in the month of june(6)
rech_6 = pd.DataFrame(high_val[rech_6_col])

# adding some other columns describing data usage of customer in june(6)
vol_col = high_val[["vol_2g_mb_6",'vol_3g_mb_6']]

rech_6 = pd.concat([rech_6,vol_col], axis = 1) 
rech_6.head(10)

From the above table we can see that whenever the date of last recharge is missing, all the corresponding values for max_rech, count_rech_2g & 3g are missing. The mobile internet usage(2G & 3G data) corresponding to these values is zero as seen from the above table. Thus let us impute these missing values by zero, considering there were no recharges done by the customer.

Recharge columns for month- 7(July)

[ ]
  1
  2
  3
# Similarly Let us observe missing values in recharge columns for the month of july(7)

rech_7_col
['total_rech_num_7',
 'total_rech_amt_7',
 'max_rech_amt_7',
 'date_of_last_rech_7',
 'date_of_last_rech_data_7',
 'max_rech_data_7',
 'count_rech_2g_7',
 'count_rech_3g_7',
 'total_data_rech_7']
[ ]
  1
  2
  3
  4
  5
  6
  7
rech_7 = pd.DataFrame(high_val[rech_7_col])

# adding some other columns describing data usage of customer in july
vol_col = high_val[["vol_2g_mb_7",'vol_3g_mb_7']]

rech_7 = pd.concat([rech_7,vol_col], axis = 1) 
rech_7.head(10)

From the above table we can see that whenever the date of last recharge is missing, all the corresponding values for max_rech, count_rech_2g & 3g are missing. The mobile internet usage(2G & 3G data) corresponding to these values is zero as seen from the above table. Thus let us impute these missing values by zero,Considering there were no recharges done by the customer. This is similar to the pattern observed for june month.

Recharge columns for month- 8(August)

[ ]
  1
  2
  3
# Similarly Let us observe missing values in recharge columns in the month of August(8)

rech_8_col
['total_rech_num_8',
 'total_rech_amt_8',
 'max_rech_amt_8',
 'date_of_last_rech_8',
 'date_of_last_rech_data_8',
 'max_rech_data_8',
 'count_rech_2g_8',
 'count_rech_3g_8',
 'total_data_rech_8']
[ ]
  1
  2
  3
  4
  5
  6
  7
rech_8 = pd.DataFrame(high_val[rech_8_col])

# adding some other columns describing data usage of customer in August
vol_col = high_val[["vol_2g_mb_8",'vol_3g_mb_8']]

rech_8 = pd.concat([rech_8,vol_col], axis = 1) 
rech_8.head(10)

Similar to june & july month, from the above table we can see that whenever the date of last recharge is missing, all the corresponding values for max_rech, count_rech_2g & 3g are missing. The mobile internet usage(2G & 3G data) corresponding to these values is zero as seen from the above table. Thus let us impute these missing values by zero.,considering there were no recharges done by the customer.This is exactly similar to the pattern observed for june & july month.

[ ]
  1
  2
  3
  4
  5
  6
  7
# let us impue the missing values in these recharge columns by zero.

impute_0 = [ 'date_of_last_rech_data_6','max_rech_data_6','count_rech_2g_6','count_rech_3g_6','total_data_rech_6',
           'date_of_last_rech_data_7','max_rech_data_7','count_rech_2g_7','count_rech_3g_7','total_data_rech_7',
           'date_of_last_rech_data_8','max_rech_data_8','count_rech_2g_8','count_rech_3g_8','total_data_rech_8']

high_val[impute_0] = high_val[impute_0].apply(lambda x: x.fillna(0))
[ ]
  1
  2
  3
  4
# Now Checking Null values
null = round(100*(high_val.isnull().sum()/len(high_val.index)),2).sort_values(ascending = False)
null = null[null!=0]
null
fb_user_8              46.83
night_pck_user_8       46.83
arpu_2g_8              46.83
arpu_3g_8              46.83
night_pck_user_6       44.15
arpu_2g_6              44.15
fb_user_6              44.15
arpu_3g_6              44.15
arpu_3g_7              43.15
night_pck_user_7       43.15
arpu_2g_7              43.15
fb_user_7              43.15
loc_og_t2m_mou_8        3.91
loc_ic_t2t_mou_8        3.91
loc_og_t2f_mou_8        3.91
spl_ic_mou_8            3.91
std_ic_t2t_mou_8        3.91
loc_og_t2c_mou_8        3.91
std_og_mou_8            3.91
loc_og_mou_8            3.91
std_og_t2t_mou_8        3.91
std_og_t2f_mou_8        3.91
std_ic_t2m_mou_8        3.91
std_ic_t2f_mou_8        3.91
std_og_t2m_mou_8        3.91
offnet_mou_8            3.91
loc_ic_t2m_mou_8        3.91
onnet_mou_8             3.91
loc_ic_t2f_mou_8        3.91
isd_og_mou_8            3.91
og_others_8             3.91
std_ic_mou_8            3.91
roam_ic_mou_8           3.91
spl_og_mou_8            3.91
roam_og_mou_8           3.91
isd_ic_mou_8            3.91
loc_ic_mou_8            3.91
loc_og_t2t_mou_8        3.91
ic_others_8             3.91
date_of_last_rech_8     1.94
std_ic_t2f_mou_6        1.82
ic_others_6             1.82
std_og_mou_6            1.82
std_ic_mou_6            1.82
spl_ic_mou_6            1.82
isd_og_mou_6            1.82
loc_ic_t2t_mou_6        1.82
og_others_6             1.82
spl_og_mou_6            1.82
loc_ic_t2m_mou_6        1.82
loc_og_t2c_mou_6        1.82
std_og_t2f_mou_6        1.82
loc_og_t2m_mou_6        1.82
loc_ic_t2f_mou_6        1.82
onnet_mou_6             1.82
offnet_mou_6            1.82
roam_ic_mou_6           1.82
loc_ic_mou_6            1.82
roam_og_mou_6           1.82
std_og_t2m_mou_6        1.82
loc_og_t2t_mou_6        1.82
isd_ic_mou_6            1.82
std_ic_t2t_mou_6        1.82
loc_og_t2f_mou_6        1.82
loc_og_mou_6            1.82
std_ic_t2m_mou_6        1.82
std_og_t2t_mou_6        1.82
loc_ic_mou_7            1.79
std_ic_t2t_mou_7        1.79
std_ic_t2f_mou_7        1.79
loc_ic_t2t_mou_7        1.79
loc_ic_t2f_mou_7        1.79
std_ic_t2m_mou_7        1.79
loc_ic_t2m_mou_7        1.79
spl_ic_mou_7            1.79
std_ic_mou_7            1.79
std_og_t2f_mou_7        1.79
isd_ic_mou_7            1.79
std_og_t2t_mou_7        1.79
onnet_mou_7             1.79
offnet_mou_7            1.79
roam_ic_mou_7           1.79
roam_og_mou_7           1.79
loc_og_t2t_mou_7        1.79
loc_og_t2m_mou_7        1.79
loc_og_t2f_mou_7        1.79
loc_og_t2c_mou_7        1.79
ic_others_7             1.79
loc_og_mou_7            1.79
std_og_t2m_mou_7        1.79
std_og_mou_7            1.79
isd_og_mou_7            1.79
spl_og_mou_7            1.79
og_others_7             1.79
date_of_last_rech_7     0.33
date_of_last_rech_6     0.24
dtype: float64
Imputing Categorical columns
[ ]
1234
# Let us impute the missing values in categorical columns by (-1). 
high_val[cat_cols] = high_val[cat_cols].apply(lambda x: x.fillna(-1)) 
high_val[cat_cols] = high_val[cat_cols].astype('str')
high_val[cat_cols].head()

[ ]
1234
# Now Checking Null values
null = round(100*(high_val.isnull().sum()/len(high_val.index)),2).sort_values(ascending = False)
null = null[null!=0]
null
arpu_3g_8              46.83
arpu_2g_8              46.83
arpu_3g_6              44.15
arpu_2g_6              44.15
arpu_2g_7              43.15
arpu_3g_7              43.15
loc_og_mou_8            3.91
std_og_mou_8            3.91
ic_others_8             3.91
spl_og_mou_8            3.91
loc_ic_t2f_mou_8        3.91
std_og_t2t_mou_8        3.91
std_ic_mou_8            3.91
loc_og_t2c_mou_8        3.91
std_ic_t2f_mou_8        3.91
isd_og_mou_8            3.91
loc_og_t2f_mou_8        3.91
loc_ic_t2m_mou_8        3.91
std_og_t2f_mou_8        3.91
loc_og_t2m_mou_8        3.91
std_og_t2m_mou_8        3.91
loc_ic_mou_8            3.91
isd_ic_mou_8            3.91
std_ic_t2t_mou_8        3.91
offnet_mou_8            3.91
loc_og_t2t_mou_8        3.91
og_others_8             3.91
loc_ic_t2t_mou_8        3.91
roam_ic_mou_8           3.91
spl_ic_mou_8            3.91
roam_og_mou_8           3.91
std_ic_t2m_mou_8        3.91
onnet_mou_8             3.91
date_of_last_rech_8     1.94
ic_others_6             1.82
std_og_mou_6            1.82
loc_ic_t2m_mou_6        1.82
og_others_6             1.82
loc_ic_t2t_mou_6        1.82
isd_og_mou_6            1.82
loc_ic_t2f_mou_6        1.82
spl_og_mou_6            1.82
spl_ic_mou_6            1.82
isd_ic_mou_6            1.82
loc_ic_mou_6            1.82
loc_og_t2t_mou_6        1.82
offnet_mou_6            1.82
std_ic_t2m_mou_6        1.82
roam_og_mou_6           1.82
onnet_mou_6             1.82
std_og_t2f_mou_6        1.82
loc_og_t2m_mou_6        1.82
std_ic_t2f_mou_6        1.82
loc_og_t2f_mou_6        1.82
std_ic_t2t_mou_6        1.82
loc_og_t2c_mou_6        1.82
loc_og_mou_6            1.82
std_ic_mou_6            1.82
std_og_t2t_mou_6        1.82
std_og_t2m_mou_6        1.82
roam_ic_mou_6           1.82
std_ic_t2t_mou_7        1.79
loc_ic_mou_7            1.79
loc_ic_t2f_mou_7        1.79
isd_ic_mou_7            1.79
spl_ic_mou_7            1.79
std_ic_t2m_mou_7        1.79
std_ic_t2f_mou_7        1.79
std_ic_mou_7            1.79
ic_others_7             1.79
std_og_mou_7            1.79
loc_ic_t2m_mou_7        1.79
std_og_t2t_mou_7        1.79
onnet_mou_7             1.79
offnet_mou_7            1.79
roam_ic_mou_7           1.79
roam_og_mou_7           1.79
loc_og_t2t_mou_7        1.79
loc_og_t2m_mou_7        1.79
loc_og_t2f_mou_7        1.79
loc_og_t2c_mou_7        1.79
loc_ic_t2t_mou_7        1.79
loc_og_mou_7            1.79
std_og_t2m_mou_7        1.79
std_og_t2f_mou_7        1.79
isd_og_mou_7            1.79
spl_og_mou_7            1.79
og_others_7             1.79
date_of_last_rech_7     0.33
date_of_last_rech_6     0.24
dtype: float64
Dropping columns with missing values more than 40%
The average revenue per user for both the phases - good & action phase, have more than 40% missing values. Since our study revolves around the identification of customers that are likely to churn taking into account only usage based churn, we can safely drop these columns without affecting our study.

[ ]
  1
  2
  3
  4
  5
  6
  7
miss = round(100*(high_val.isnull().sum()/len(high_val.index)),2).sort_values(ascending = False) 

# Let us not delete data from 9th month
miss = pd.DataFrame(miss[miss >= 40])
threshold_col = miss.index

high_val = high_val.drop(threshold_col,1)
[ ]
  1
high_val.shape
(30001, 155)
Let us observe outgoing minutes of usage for local, standard, special and others
month-june(6)

[ ]
  1
  2
  3
  4
  5
# From the business domain knowledge, we know that total_og column is the addition of local , standard, special andd other outgoing calls
og_call_6 = ['loc_og_mou_6','std_og_mou_6','isd_og_mou_6','spl_og_mou_6','og_others_6','total_og_mou_6']
total_og_6 = high_val[og_call_6]
# filtering only those clients who have made no outgoing calls
total_og_6.loc[total_og_6['total_og_mou_6']==0].head()

Thus, we can coclude from the business domain knowledge and above table that when the total outgoing calls are zero, no outgoing calls have been made in that month. We can impute the missing values in the columns of local outgoing calls, standard outgoing calls, special outgoing calls & other outgoing calls by 0.

[ ]
  1
  2
  3
  4
#Lets verify is our assumption right or not, sum of all outgoing calls is equal to the the tota_og_mou_6

df['outgoing_total_6'] = df['loc_og_mou_6']+ df['std_og_mou_6']+df['isd_og_mou_6']+df['spl_og_mou_6']+df['og_others_6']
df[['outgoing_total_6','total_og_mou_6']].dropna().corr()

From above we can clearly see that these are correlated and hence when the total outgoing calls are zero, no outgoing calls have been made in that month. We can impute the missing values in the columns of local outgoing calls, standard outgoing calls, special outgoing calls & other outgoing calls by 0.

[ ]
  1
  2
# Imputing by '0'
high_val[og_call_6] = high_val[og_call_6].fillna(0)
From the dataset, it is evident that local outgoing calls in any month are equal to the sum of all the types of local outgoing calls i.e. (T2T,T2M,T2F & T2C)

[ ]
  1
  2
  3
  4
  5
og_loc_6 = ['loc_og_t2t_mou_6','loc_og_t2m_mou_6','loc_og_t2f_mou_6','loc_og_t2c_mou_6','loc_og_mou_6']
loc_og_6 = high_val[og_loc_6]

# filtering only those clients who have made no local outgoing calls
loc_og_6.loc[loc_og_6 ['loc_og_mou_6']==0].head()

If the total local outgoing calls made in any month are zero, then it means that no type of local calls were made i.e. T2T,T2M,T2F & T2C . We can replace the missing values corresponding to them by '0'

[ ]
  1
  2
  3
  4
  5
#Lets verify is our assumption right or not, sum of all local outgoing calls is equal to the the total local outgoing call
#"loc_og_mou_6"

df['loc_outgoing_total_6'] = df['loc_og_t2t_mou_6']+ df['loc_og_t2m_mou_6']+df['loc_og_t2f_mou_6']+df['loc_og_t2c_mou_6']
df[['loc_outgoing_total_6','loc_og_mou_6']].dropna().corr().round()

From above we can clearly see that these are correlated and hence if the total local outgoing calls made in any month are zero, then it means that no type of local calls were made i.e. T2T,T2M,T2F & T2C

[ ]
  1
  2
# Imputing by '0'
high_val[og_loc_6] = high_val[og_loc_6].fillna(0)
From the dataset, it is evident that std outgoing calls in any month are equal to the sum of all the types of std outgoing calls i.e. (T2T,T2M,T2F & T2C)

[ ]
  1
  2
  3
og_std_6 = ['std_og_t2t_mou_6','std_og_t2m_mou_6','std_og_t2f_mou_6','std_og_mou_6']
std_og_6 = high_val[og_std_6]
std_og_6.head()

If the total standard outgoing calls made in any month are zero, then it means that no type of std calls were made i.e. T2T,T2M,T2F & T2C . We can replace the missing values corresponding to them by '0'

[ ]
  1
  2
  3
  4
  5
#Lets verify is our assumption right or not, sum of all std outgoing calls is equal to the the total std outgoing call
#"std_og_mou_6"

df['std_outgoing_total_6'] = df['std_og_t2t_mou_6']+ df['std_og_t2m_mou_6']+df['std_og_t2f_mou_6']
df[['std_outgoing_total_6','std_og_mou_6']].dropna().corr().round()

From above we can clearly see that these are correlated and hence if the total std outgoing calls made in any month are zero, then it means that no type of std calls were made i.e. T2T,T2M,T2F & T2C

[ ]
  1
  2
# Imputing by '0'
high_val[og_std_6] = high_val[og_std_6].fillna(0)
month-july(7)

[ ]
  1
  2
  3
  4
  5
# From the business domain knowledge, we know that total_og column is the addition of local , standard, special and other outgoing calls
og_call_7 = ['loc_og_mou_7','std_og_mou_7','isd_og_mou_7','spl_og_mou_7','og_others_7','total_og_mou_7']
total_og_7 = high_val[og_call_7]
# filtering only those clients who have made no outgoing calls
total_og_7.loc[total_og_7['total_og_mou_7']==0].head()

Thus, we can coclude from the business domain knowledge and above table that when the total outgoing calls are zero, no outgoing calls have been made in that month. We can impute the missing values in the columns of local outgoing calls, standard outgoing calls, special outgoing calls & other outgoing calls by 0.

[ ]
  1
  2
  3
  4
#Lets verify is our assumption right or not, sum of all outgoing calls is equal to the the tota_og_mou_7

df['outgoing_total_7'] = df['loc_og_mou_7']+ df['std_og_mou_7']+df['isd_og_mou_7']+df['spl_og_mou_7']+df['og_others_7']
df[['outgoing_total_7','total_og_mou_7']].dropna().corr()

From above we can clearly see that these are correlated and hence when the total outgoing calls are zero, no outgoing calls have been made in that month. We can impute the missing values in the columns of local outgoing calls, standard outgoing calls, special outgoing calls & other outgoing calls by 0.

[ ]
  1
  2
# Imputing by '0'
high_val[og_call_7] = high_val[og_call_7].fillna(0)
From the dataset, it is evident that local outgoing calls in any month are equal to the sum of all the types of local outgoing calls i.e. (T2T,T2M,T2F & T2C)

[ ]
  1
  2
  3
  4
  5
og_loc_7 = ['loc_og_t2t_mou_7','loc_og_t2m_mou_7','loc_og_t2f_mou_7','loc_og_t2c_mou_7','loc_og_mou_7']
loc_og_7 = high_val[og_loc_7]

# filtering only those clients who have made no local outgoing calls
loc_og_7.loc[loc_og_7 ['loc_og_mou_7']==0].head()

If the total local outgoing calls made in any month are zero, then it means that no type of local calls were made i.e. T2T,T2M,T2F & T2C . We can replace the missing values corresponding to them by '0'

[ ]
  1
  2
  3
  4
  5
#Lets verify is our assumption right or not, sum of all local outgoing calls is equal to the the total local outgoing call
#"loc_og_mou_7"

df['loc_outgoing_total_7'] = df['loc_og_t2t_mou_7']+ df['loc_og_t2m_mou_7']+df['loc_og_t2f_mou_7']+df['loc_og_t2c_mou_7']
df[['loc_outgoing_total_7','loc_og_mou_7']].dropna().corr().round()

From above we can clearly see that these are correlated and hence if the total local outgoing calls made in any month are zero, then it means that no type of local calls were made i.e. T2T,T2M,T2F & T2C

[ ]
  1
  2
# Imputing by '0'
high_val[og_loc_7] = high_val[og_loc_7].fillna(0)
From the dataset, it is evident that std outgoing calls in any month are equal to the sum of all the types of std outgoing calls i.e. (T2T,T2M,T2F & T2C)

[ ]
  1
  2
  3
og_std_7 = ['std_og_t2t_mou_7','std_og_t2m_mou_7','std_og_t2f_mou_7','std_og_mou_7']
std_og_7 = high_val[og_std_7]
std_og_7.head()

If the total standard outgoing calls made in any month are zero, then it means that no type of std calls were made i.e. T2T,T2M,T2F & T2C . We can replace the missing values corresponding to them by '0'

[ ]
  1
  2
  3
  4
  5
#Lets verify is our assumption right or not, sum of all std outgoing calls is equal to the the total std outgoing call
#"std_og_mou_7"

df['std_outgoing_total_7'] = df['std_og_t2t_mou_7']+ df['std_og_t2m_mou_7']+df['std_og_t2f_mou_7']
df[['std_outgoing_total_7','std_og_mou_7']].dropna().corr()

From above we can clearly see that these are correlated and hence if the total std outgoing calls made in any month are zero, then it means that no type of std calls were made i.e. T2T,T2M,T2F & T2C

[ ]
  1
  2
# Imputing by '0'
high_val[og_std_7] = high_val[og_std_7].fillna(0)
month-Aug(8)

[ ]
  1
  2
  3
  4
  5
# From the business domain knowledge, we know that total_og column is the addition of local , standard, special and other outgoing calls
og_call_8 = ['loc_og_mou_8','std_og_mou_8','isd_og_mou_8','spl_og_mou_8','og_others_8','total_og_mou_8']
total_og_8 = high_val[og_call_8]
# filtering only those clients who have made no outgoing calls
total_og_8.loc[total_og_8['total_og_mou_8']==0].head()

Thus, we can coclude from the business domain knowledge and above table that when the total outgoing calls are zero, no outgoing calls have been made in that month. We can impute the missing values in the columns of local outgoing calls, standard outgoing calls, special outgoing calls & other outgoing calls by 0.

[ ]
  1
  2
  3
  4
#Lets verify is our assumption right or not, sum of all outgoing calls is equal to the the tota_og_mou_8

df['outgoing_total_8'] = df['loc_og_mou_8']+ df['std_og_mou_8']+df['isd_og_mou_8']+df['spl_og_mou_8']+df['og_others_8']
df[['outgoing_total_8','total_og_mou_8']].dropna().corr()

From above we can clearly see that these are correlated and hence when the total outgoing calls are zero, no outgoing calls have been made in that month. We can impute the missing values in the columns of local outgoing calls, standard outgoing calls, special outgoing calls & other outgoing calls by 0.

[ ]
  1
  2
# Imputing by '0'
high_val[og_call_8] = high_val[og_call_8].fillna(0)
From the dataset, it is evident that local outgoing calls in any month are equal to the sum of all the types of local outgoing calls i.e. (T2T,T2M,T2F & T2C)

[ ]
  1
  2
  3
  4
  5
og_loc_8 = ['loc_og_t2t_mou_8','loc_og_t2m_mou_8','loc_og_t2f_mou_8','loc_og_t2c_mou_8','loc_og_mou_8']
loc_og_8 = high_val[og_loc_8]

# filtering only those clients who have made no local outgoing calls
loc_og_8.loc[loc_og_8 ['loc_og_mou_8']==0].head()

If the total local outgoing calls made in any month are zero, then it means that no type of local calls were made i.e. T2T,T2M,T2F & T2C . We can replace the missing values corresponding to them by '0'

[ ]
  1
  2
  3
  4
  5
#Lets verify is our assumption right or not, sum of all local outgoing calls is equal to the the total local outgoing call
#"loc_og_mou_8"

df['loc_outgoing_total_8'] = df['loc_og_t2t_mou_8']+ df['loc_og_t2m_mou_8']+df['loc_og_t2f_mou_8']+df['loc_og_t2c_mou_8']
df[['loc_outgoing_total_8','loc_og_mou_8']].dropna().corr().round()

From above we can clearly see that these are correlated and hence if the total local outgoing calls made in any month are zero, then it means that no type of local calls were made i.e. T2T,T2M,T2F & T2C

[ ]
  1
  2
# Imputing by '0'
high_val[og_loc_8] = high_val[og_loc_8].fillna(0)
From the dataset, it is evident that std outgoing calls in any month are equal to the sum of all the types of std outgoing calls i.e. (T2T,T2M,T2F & T2C)

[ ]
  1
  2
  3
og_std_8 = ['std_og_t2t_mou_8','std_og_t2m_mou_8','std_og_t2f_mou_8','std_og_mou_8']
std_og_8 = high_val[og_std_8]
std_og_8.head()

If the total standard outgoing calls made in any month are zero, then it means that no type of std calls were made i.e. T2T,T2M,T2F & T2C . We can replace the missing values corresponding to them by '0'

[ ]
  1
  2
  3
  4
  5
#Lets verify is our assumption right or not, sum of all std outgoing calls is equal to the the total std outgoing call
#"std_og_mou_8"

df['std_outgoing_total_8'] = df['std_og_t2t_mou_8']+ df['std_og_t2m_mou_8']+df['std_og_t2f_mou_8']
df[['std_outgoing_total_8','std_og_mou_8']].dropna().corr()

From above we can clearly see that these are correlated and hence if the total std outgoing calls made in any month are zero, then it means that no type of std calls were made i.e. T2T,T2M,T2F & T2C

[ ]
  1
  2
# Imputing by '0'
high_val[og_std_8] = high_val[og_std_8].fillna(0)
Let us observe Incoming minutes of usage for local, standard, special and others
month-june(6)

[ ]
  1
  2
  3
  4
  5
# From the business domain knowledge, we know that total_ic column is the addition of local , standard, special andd other incoming calls
ic_call_6 = ['loc_ic_mou_6','std_ic_mou_6','isd_ic_mou_6','spl_ic_mou_6','ic_others_6','total_ic_mou_6']
total_ic_6 = high_val[ic_call_6]
# filtering only those clients who have made no outgoing calls
total_ic_6 .loc[total_ic_6 ['total_ic_mou_6']==0].head()

Thus, we can coclude from the business domain knowledge and above table that when the total incoming calls are zero, no incoming calls are recieved in that month. We can impute the missing values in the columns of local incoming calls, standard incoming calls, special incoming calls & other incoming calls by 0.

[ ]
  1
  2
  3
  4
#Lets verify is our assumption right or not, sum of all incoming calls is equal to the the tota_ic_mou_6

df['incoming_total_6'] = df['loc_ic_mou_6']+ df['std_ic_mou_6']+df['isd_ic_mou_6']+df['spl_ic_mou_6']+df['ic_others_6']
df[['incoming_total_6','total_ic_mou_6']].dropna().corr()

From above we can clearly see that these are correlated and hence when the total incoming calls are zero, no outgoing calls have been made in that month. We can impute the missing values in the columns of local outgoing calls, standard outgoing calls, special incoming calls & other outgoing calls by 0.

[ ]
  1
  2
# Imputing by '0'
high_val[ic_call_6] = high_val[ic_call_6].fillna(0)
From the dataset, it is evident that local incoming calls in any month are equal to the sum of all the types of local incoming calls i.e. (T2T,T2M,T2F & T2C)

[ ]
  1
  2
  3
  4
  5
ic_loc_6 = ['loc_ic_t2t_mou_6','loc_ic_t2m_mou_6','loc_ic_t2f_mou_6','loc_ic_mou_6']
loc_ic_6= high_val[ic_loc_6]

# filtering only those clients who have made no local outgoing calls
loc_ic_6.loc[loc_ic_6['loc_ic_mou_6']==0].head()

If the total local incomingcalls made in any month are zero, then it means that no type of local calls were recieved i.e. T2T,T2M,T2F & T2C . We can replace the missing values corresponding to them by '0'

[ ]
  1
  2
  3
  4
  5
#Lets verify is our assumption right or not, sum of all local outgoing calls is equal to the the total local outgoing call
#"loc_ic_mou_6"

df['loc_incoming_total_6'] = df['loc_ic_t2t_mou_6']+ df['loc_ic_t2m_mou_6']+df['loc_ic_t2f_mou_6']
df[['loc_incoming_total_6','loc_ic_mou_6']].dropna().corr()

From above we can clearly see that these are correlated and hence if the total local outgoing calls made in any month are zero, then it means that no type of local calls were made i.e. T2T,T2M,T2F & T2C

[ ]
  1
  2
# Imputing by '0'
high_val[ic_loc_6] = high_val[ic_loc_6].fillna(0)
From the dataset, it is evident that std incoming calls in any month are equal to the sum of all the types of std incoming calls i.e. (T2T,T2M,T2F & T2C)

[ ]
  1
  2
  3
ic_std_6 = ['std_ic_t2t_mou_6','std_ic_t2m_mou_6','std_ic_t2f_mou_6','std_ic_mou_6']
std_ic_6 = high_val[ic_std_6]
std_ic_6.head()

If the total standard outgoing calls made in any month are zero, then it means that no type of std calls were made i.e. T2T,T2M,T2F & T2C . We can replace the missing values corresponding to them by '0'

[ ]
12345
#Lets verify is our assumption right or not, sum of all std outgoing calls is equal to the the total std outgoing call
#"std_ic_mou_6"

df['std_incoming_total_6'] = df['std_ic_t2t_mou_6']+ df['std_ic_t2m_mou_6']+df['std_ic_t2f_mou_6']
df[['std_incoming_total_6','std_ic_mou_6']].dropna().corr()

From above we can clearly see that these are correlated and hence if the total std outgoing calls made in any month are zero, then it means that no type of std calls were made i.e. T2T,T2M,T2F & T2C

[ ]
12
# Imputing by '0'
high_val[ic_std_6] = high_val[ic_std_6].fillna(0)
month-july(7)

[ ]
12345
# From the business domain knowledge, we know that total_ic column is the addition of local , standard, special andd other incoming calls
ic_call_7 = ['loc_ic_mou_7','std_ic_mou_7','isd_ic_mou_7','spl_ic_mou_7','ic_others_7','total_ic_mou_7']
total_ic_7 = high_val[ic_call_7]
# filtering only those clients who have made no outgoing calls
total_ic_7 .loc[total_ic_7 ['total_ic_mou_7']==0].head()

Thus, we can conclude from the business domain knowledge and above table that when the total incoming calls are zero, no incoming calls are recieved in that month. We can impute the missing values in the columns of local incoming calls, standard incoming calls, special incoming calls & other incoming calls by 0.

[ ]
  1
  2
  3
  4
  5
#Lets verify is our assumption right or not, sum of all local incoming calls is equal to the the total local incoming call
#"loc_ic_mou_7"

df['loc_incoming_total_7'] = df['loc_ic_mou_7']+ df['std_ic_mou_7']+df['isd_ic_mou_7']+df['spl_ic_mou_7']+df['ic_others_7']
df[['loc_incoming_total_7','total_ic_mou_7']].dropna().corr()

From above we can clearly see that these are correlated and hence if the total local outgoing calls made in any month are zero, then it means that no type of local calls were made i.e. T2T,T2M,T2F & T2C

[ ]
  1
  2
# Imputing by '0'
high_val[ic_call_7] = high_val[ic_call_7].fillna(0)
From the dataset, it is evident that local incoming calls in any month are equal to the sum of all the types of local incoming calls i.e. (T2T,T2M,T2F & T2C)

[ ]
  1
  2
  3
  4
  5
ic_loc_7 = ['loc_ic_t2t_mou_7','loc_ic_t2m_mou_7','loc_ic_t2f_mou_7','loc_ic_mou_7']
loc_ic_7= high_val[ic_loc_7]

# filtering only those clients who have made no local outgoing calls
loc_ic_7.loc[loc_ic_7['loc_ic_mou_7']==0].head()

If the total local incomingcalls made in any month are zero, then it means that no type of local calls were recieved i.e. T2T,T2M,T2F & T2C . We can replace the missing values corresponding to them by '0'

[ ]
  1
  2
  3
  4
  5
#Lets verify is our assumption right or not, sum of all local incoming calls is equal to the the total local incoming call
#"loc_ic_mou_7"

df['loc_incoming_total_7'] = df['loc_ic_t2t_mou_7']+ df['loc_ic_t2m_mou_7']+df['loc_ic_t2f_mou_7']
df[['loc_incoming_total_7','loc_ic_mou_7']].dropna().corr()

From above we can clearly see that these are correlated and hence if the total local outgoing calls made in any month are zero, then it means that no type of local calls were made i.e. T2T,T2M,T2F & T2C

[ ]
  1
  2
# Imputing by '0'
high_val[ic_loc_7] = high_val[ic_loc_7].fillna(0)
From the dataset, it is evident that std incoming calls in any month are equal to the sum of all the types of std incoming calls i.e. (T2T,T2M,T2F & T2C)

[ ]
  1
  2
  3
ic_std_7 = ['std_ic_t2t_mou_7','std_ic_t2m_mou_7','std_ic_t2f_mou_7','std_ic_mou_7']
std_ic_7 = high_val[ic_std_7]
std_ic_7.head()

If the total standard outgoing calls made in any month are zero, then it means that no type of std calls were made i.e. T2T,T2M,T2F & T2C . We can replace the missing values corresponding to them by '0'

[ ]
  1
  2
  3
  4
  5
#Lets verify is our assumption right or not, sum of all local incoming calls is equal to the the total local incoming call
#"std_ic_mou_7"

df['std_incoming_total_7'] = df['std_ic_t2t_mou_7']+ df['std_ic_t2m_mou_7']+df['std_ic_t2f_mou_7']
df[['std_incoming_total_7','std_ic_mou_7']].dropna().corr()

From above we can clearly see that these are correlated and hence if the total local outgoing calls made in any month are zero, then it means that no type of local calls were made i.e. T2T,T2M,T2F & T2C

[ ]
  1
  2
# Imputing by '0'
high_val[ic_std_7] = high_val[ic_std_7].fillna(0)
month-Aug(8)

[ ]
  1
  2
  3
  4
  5
# From the business domain knowledge, we know that total_ic column is the addition of local , standard, special andd other incoming calls
ic_call_8 = ['loc_ic_mou_8','std_ic_mou_8','isd_ic_mou_8','spl_ic_mou_8','ic_others_8','total_ic_mou_8']
total_ic_8 = high_val[ic_call_8]
# filtering only those clients who have made no outgoing calls
total_ic_8 .loc[total_ic_8['total_ic_mou_8']==0].head()

Thus, we can coclude from the business domain knowledge and above table that when the total incoming calls are zero, no incoming calls are recieved in that month. We can impute the missing values in the columns of local incoming calls, standard incoming calls, special incoming calls & other incoming calls by 0.

[ ]
  1
  2
  3
  4
#Lets verify is our assumption right or not, sum of all incoming calls is equal to the the tota_ic_mou_8

df['incoming_total_8'] = df['loc_ic_mou_8']+ df['std_ic_mou_8']+df['isd_ic_mou_8']+df['spl_ic_mou_8']+df['ic_others_8']
df[['incoming_total_8','total_ic_mou_8']].dropna().corr()

From above we can clearly see that these are correlated and hence when the total incoming calls are zero, no outgoing calls have been made in that month. We can impute the missing values in the columns of local outgoing calls, standard outgoing calls, special incoming calls & other outgoing calls by 0.

[ ]
  1
  2
# Imputing by '0'
high_val[ic_call_8] = high_val[ic_call_8].fillna(0)
From the dataset, it is evident that local incoming calls in any month are equal to the sum of all the types of local incoming calls i.e. (T2T,T2M,T2F & T2C)

[ ]
  1
  2
  3
  4
  5
ic_loc_8 = ['loc_ic_t2t_mou_8','loc_ic_t2m_mou_8','loc_ic_t2f_mou_8','loc_ic_mou_8']
loc_ic_8= high_val[ic_loc_8]

# filtering only those clients who have made no local outgoing calls
loc_ic_8.loc[loc_ic_8['loc_ic_mou_8']==0].head()

If the total local incomingcalls made in any month are zero, then it means that no type of local calls were recieved i.e. T2T,T2M,T2F & T2C . We can replace the missing values corresponding to them by '0'

[ ]
  1
  2
  3
  4
  5
#Lets verify is our assumption right or not, sum of all local incoming calls is equal to the the total local incoming call
#"loc_ic_mou_8"

df['loc_incoming_total_8'] = df['loc_ic_t2t_mou_8']+ df['loc_ic_t2m_mou_8']+df['loc_ic_t2f_mou_8']
df[['loc_incoming_total_8','loc_ic_mou_8']].dropna().corr()

From above we can clearly see that these are correlated and hence if the total local outgoing calls made in any month are zero, then it means that no type of local calls were made i.e. T2T,T2M,T2F & T2C

[ ]
  1
  2
# Imputing by '0'
high_val[ic_loc_8] = high_val[ic_loc_8].fillna(0)
From the dataset, it is evident that std incoming calls in any month are equal to the sum of all the types of std incoming calls i.e. (T2T,T2M,T2F & T2C)

[ ]
  1
  2
  3
ic_std_8 = ['std_ic_t2t_mou_8','std_ic_t2m_mou_8','std_ic_t2f_mou_8','std_ic_mou_8']
std_ic_8 = high_val[ic_std_8]
std_ic_8.head()

If the total standard outgoing calls made in any month are zero, then it means that no type of std calls were made i.e. T2T,T2M,T2F & T2C . We can replace the missing values corresponding to them by '0'

[ ]
  1
  2
  3
  4
  5
#Lets verify is our assumption right or not, sum of all local incoming calls is equal to the the total local incoming call
#"std_ic_mou_8"

df['std_incoming_total_8'] = df['std_ic_t2t_mou_8']+ df['std_ic_t2m_mou_8']+df['std_ic_t2f_mou_8']
df[['std_incoming_total_8','std_ic_mou_8']].dropna().corr()

From above we can clearly see that these are correlated and hence if the total local outgoing calls made in any month are zero, then it means that no type of local calls were made i.e. T2T,T2M,T2F & T2C

[ ]
  1
  2
# Imputing by '0'
high_val[ic_std_8] = high_val[ic_std_8].fillna(0)
[ ]
  1
  2
  3
  4
## Let us now check the % of null values
null = round(100*(high_val.isnull().sum()/len(high_val.index)),2).sort_values(ascending = False)
null = null[null!=0]
null
offnet_mou_8           3.91
onnet_mou_8            3.91
roam_og_mou_8          3.91
roam_ic_mou_8          3.91
date_of_last_rech_8    1.94
roam_ic_mou_6          1.82
offnet_mou_6           1.82
roam_og_mou_6          1.82
onnet_mou_6            1.82
onnet_mou_7            1.79
offnet_mou_7           1.79
roam_ic_mou_7          1.79
roam_og_mou_7          1.79
date_of_last_rech_7    0.33
date_of_last_rech_6    0.24
dtype: float64
Drop Rows with all Null Values
[ ]
  1
high_val = high_val.dropna(how='all',axis=0) 
[ ]
  1
high_val.shape
(30001, 155)
[ ]
  1
  2
  3
  4
# Now Checking Null values
null = round(100*(high_val.isnull().sum()/len(high_val.index)),2).sort_values(ascending = False)
null = null[null!=0]
null
offnet_mou_8           3.91
onnet_mou_8            3.91
roam_og_mou_8          3.91
roam_ic_mou_8          3.91
date_of_last_rech_8    1.94
roam_ic_mou_6          1.82
offnet_mou_6           1.82
roam_og_mou_6          1.82
onnet_mou_6            1.82
onnet_mou_7            1.79
offnet_mou_7           1.79
roam_ic_mou_7          1.79
roam_og_mou_7          1.79
date_of_last_rech_7    0.33
date_of_last_rech_6    0.24
dtype: float64
Dropping date columns
[ ]
  1
  2
  3
# Let us drop the date columns as they do not infer anything 
del_date = [i for i in high_val.columns if 'date' in i]
high_val= high_val.drop(del_date,1)
[ ]
  1
high_val.shape
(30001, 149)
[ ]
  1
  2
  3
  4
# Now Checking Null values
null = round(100*(high_val.isnull().sum()/len(high_val.index)),2).sort_values(ascending = False)
null = null[null!=0]
null
roam_ic_mou_8    3.91
offnet_mou_8     3.91
roam_og_mou_8    3.91
onnet_mou_8      3.91
roam_og_mou_6    1.82
onnet_mou_6      1.82
offnet_mou_6     1.82
roam_ic_mou_6    1.82
roam_og_mou_7    1.79
roam_ic_mou_7    1.79
offnet_mou_7     1.79
onnet_mou_7      1.79
dtype: float64
Replacing the remaining missing values
Let us replace NaN values with mean for all other columns where missing value percentage is less than 4%. Most of the data is clean and and replacing thse remaining values by mean will not impact the analysis.

[ ]
  1
high_val[null.index].describe()

[ ]
  1
  2
  3
miss_col = null.index
for i in miss_col:
    high_val[i] = high_val[i].fillna(high_val[i].mean())
[ ]
  1
  2
  3
  4
  5
## Let us now check the % of null values

null = round(100*(high_val.isnull().sum()/len(high_val.index)),2).sort_values(ascending = False)
null = null[null!=0]
null
Series([], dtype: float64)
There are no misssing values present in our dataset.

[ ]
  1
high_val.shape
(30001, 149)
[ ]
  1
high_val.head()

7. Univariate Analysis
a.) Data Distribution

Let us chech the dataset for skewness as skewed dataset gives biased result.

[ ]
  1
  2
  3
  4
df =pd.DataFrame(high_val.drop('cust_id',1).skew().sort_values(ascending = False))
df.columns = ['Skewness']
df


[ ]
  1
  2
  3
  4
  5
# let us filter those columns which have lot of skewness.  
# Let us select 40 as a threshold to eliminate maximum skewness.

skew = df['Skewness'].loc[lambda x: x>=40].index
skew
Index(['og_others_7', 'og_others_6', 'og_others_8', 'isd_og_mou_8',
       'isd_og_mou_6', 'isd_og_mou_7', 'spl_ic_mou_7', 'ic_others_6',
       'ic_others_8', 'ic_others_7', 'spl_ic_mou_6'],
      dtype='object')
[ ]
  1
  2
  3
# Let us drop these columns 
high_val.drop(skew,1,inplace=True)
high_val.shape
(30001, 138)
b.) Visualising the spread of data

Columns related to call & data recharge for good & action phase

[ ]
  1
  2
rech_call_data =  high_val.columns[high_val.columns.str.contains('rech_amt|rech_data')]
rech_call_data
Index(['total_rech_amt_6', 'total_rech_amt_7', 'total_rech_amt_8',
       'max_rech_amt_6', 'max_rech_amt_7', 'max_rech_amt_8', 'max_rech_data_6',
       'max_rech_data_7', 'max_rech_data_8', 'rech_amt_good_yr'],
      dtype='object')
[ ]
  1
  2
  3
for cols in rech_call_data:
    sns.distplot(high_val[cols], label = cols)
    plt.show()

Columns related to All kind of calls within the same operator network & outside the network for good & action phase

[ ]
  1
  2
onnet_offnet_data =  high_val.columns[high_val.columns.str.contains('onnet|offnet')]
onnet_offnet_data
Index(['onnet_mou_6', 'onnet_mou_7', 'onnet_mou_8', 'offnet_mou_6',
       'offnet_mou_7', 'offnet_mou_8'],
      dtype='object')
[ ]
  1
  2
  3
for cols in onnet_offnet_data:
    sns.distplot(high_val[cols], label = cols)
    plt.show()

Columns related to total outgoing calls & roaming outgoing calls for good & action phase

[ ]
  1
  2
og_data =  high_val.columns[high_val.columns.str.contains('total_og|roam_og')]
og_data
Index(['roam_og_mou_6', 'roam_og_mou_7', 'roam_og_mou_8', 'total_og_mou_6',
       'total_og_mou_7', 'total_og_mou_8'],
      dtype='object')
[ ]
  1
  2
  3
for cols in og_data:
    sns.distplot(high_val[cols], label = cols,kde_kws={'bw':0.1})
    plt.show()

Columns related to total incoming calls & roaming outgoing calls for good & action phase

[ ]
  1
  2
ic_data =  high_val.columns[high_val.columns.str.contains('total_ic|roam_ic')]
ic_data
Index(['roam_ic_mou_6', 'roam_ic_mou_7', 'roam_ic_mou_8', 'total_ic_mou_6',
       'total_ic_mou_7', 'total_ic_mou_8'],
      dtype='object')
[ ]
  1
  2
  3
for cols in ic_data:
    sns.distplot(high_val[cols], label = cols, kde_kws={'bw':0.1})
    plt.show()

Columns related to average revenue per user for good & action phase

[ ]
  1
  2
arpu_data =  high_val.columns[high_val.columns.str.contains('arpu')]
arpu_data
Index(['arpu_6', 'arpu_7', 'arpu_8'], dtype='object')
[ ]
  1
  2
  3
for cols in arpu_data:
    sns.distplot(high_val[cols], label = cols, kde_kws={'bw':0.1})
    plt.show()

c.) Observing Numerical columns - Outlier analysis

Outliers in all types of outgoing calls in all the months

[ ]
  1
  2
  3
  4
  5
  6
  7
for i in high_val.columns[high_val.columns.str.contains("og")]:
    plt.figure(figsize =(15,5))
    sns.boxplot(high_val[i])
    plt.xticks(rotation = 90)
    plt.title('Boxplot of '+' '+ i)
    plt.xlabel(i)
    plt.show()      

Outliers in all types of incoming calls in all the months

[ ]
  1
  2
  3
  4
  5
  6
  7
for i in high_val.columns[high_val.columns.str.contains("ic")]:
    plt.figure(figsize =(15,5))
    sns.boxplot(high_val[i])
    plt.xticks(rotation = 90)
    plt.title('Boxplot of '+' '+ i)
    plt.xlabel(i)
    plt.show()      

Outliers in monthly data recharges done in all the months

[ ]
  1
  2
  3
  4
  5
  6
  7
for i in high_val.columns[high_val.columns.str.contains("monthly")]:
        plt.figure(figsize =(15,5))
        sns.boxplot(high_val[i].astype('int'))
        plt.xticks(rotation = 90)
        plt.title('Boxplot of '+' '+ i)
        plt.xlabel(i)
        plt.show()   

Outliers in sachet recharges done in all the months

[ ]
  1
  2
  3
  4
  5
  6
  7
for i in high_val.columns[high_val.columns.str.contains("sachet")]:
        plt.figure(figsize =(15,5))
        sns.boxplot(high_val[i].astype('int'))
        plt.xticks(rotation = 90)
        plt.title('Boxplot of '+' '+ i)
        plt.xlabel(i)
        plt.show()   

Outliers in All kind of calls within the same operator network in all the months

[ ]
  1
  2
  3
  4
  5
  6
  7
for i in high_val.columns[high_val.columns.str.contains("net")]:
        plt.figure(figsize =(15,5))
        sns.boxplot(high_val[i])
        plt.xticks(rotation = 90)
        plt.title('Boxplot of '+' '+ i)
        plt.xlabel(i)
        plt.show() 

outliers in alll kind of calls outside the operator network

[ ]
  1
  2
  3
  4
  5
  6
  7
for i in high_val.columns[high_val.columns.str.contains("arpu")]:
        plt.figure(figsize =(15,5))
        sns.boxplot(high_val[i])
        plt.xticks(rotation = 90)
        plt.title('Boxplot of '+' '+ i)
        plt.xlabel(i)
        plt.show() 

d.) Observing categorical columns

[ ]
  1
  2
  3
  4
  5
  6
  7
  8
for i in cat_cols:
    if (i != 'churn')|(i != 'age_group'):
        plt.figure(figsize =(12,4))
        sns.countplot(high_val[i])
        plt.xticks(rotation = 90)
        plt.title('Target variable in'+' '+ i)
        plt.xlabel(i)
        plt.show()

[ ]
  1
  2
sns.countplot(high_val['age_group'], hue = high_val['churn'], palette="Set1")


8. Bivariate Analysis
[ ]
  1
  2
  3
  4
  5
  6
  7
def bivar_plot(var1, var2,fig):
    plt.figure(figsize = (20,15))
    plt.subplot(4,2, fig)
    if var1.dtype != 'object' and var2.dtype != 'object':
        sns.regplot(var1, var2)
    elif (var1.dtype == 'object' and var2.dtype != 'object') or (var1.dtype != 'object' and var2.dtype == 'object'):        
        sns.boxplot(var1, var2)
a.) Analyzing age of network with churn

[ ]
  1
bivar_plot(high_val.churn, high_val.aon_yr,1)

Average number of days cutomer using the network before churning is less than 1000.
b.) Analyzing onnet with churn

[ ]
  1
  2
  3
bivar_plot(high_val.churn, np.log(high_val.onnet_mou_6),1)
bivar_plot(high_val.churn, np.log(high_val.onnet_mou_7),2)
bivar_plot(high_val.churn, np.log(high_val.onnet_mou_8),3)

People who have churned, used most of the calling in moth of June and July on same network, where as in month of August there calling is redcued and it is indicating that the customers are likely to churn
c.) Analyzing offnet with churn

[ ]
  1
  2
  3
bivar_plot(high_val.churn, np.log(high_val.offnet_mou_6),1)
bivar_plot(high_val.churn, np.log(high_val.offnet_mou_7),2)
bivar_plot(high_val.churn, np.log(high_val.offnet_mou_8),3)

People who have churned, used most of the calling in month of June and July on different network, where as in month of August there calling is redcued and it is indicating that the customers are likely to churn
d.) Analyzing night pack user with churn

[ ]
  1
pd.crosstab(high_val.churn, high_val.night_pck_user_8, normalize='columns')*100

e.) Analyzing Sachet with churn

[ ]
  1
pd.crosstab(high_val.churn, high_val.sachet_3g_8)

f.) Analyzing average revenue per user with churn

[ ]
  1
  2
  3
bivar_plot(high_val.churn, np.log(high_val.arpu_6),1)
bivar_plot(high_val.churn, np.log(high_val.arpu_7),2)
bivar_plot(high_val.churn, np.log(high_val.arpu_8),3)

g.) Analyzing total recharge amount with churn

[ ]
  1
  2
  3
bivar_plot(high_val.churn, np.log(high_val.total_rech_amt_6),1)
bivar_plot(high_val.churn, np.log(high_val.total_rech_amt_7),2)
bivar_plot(high_val.churn, np.log(high_val.total_rech_amt_8),3)

h.) Analyzing total recharge number with churn

[ ]
  1
  2
  3
bivar_plot(high_val.churn, np.log(high_val.total_rech_num_6),1)
bivar_plot(high_val.churn, np.log(high_val.total_rech_num_7),2)
bivar_plot(high_val.churn, np.log(high_val.total_rech_num_8),3)

[ ]
  1

correlation among the features
[ ]
  1
  2
  3
# Observing the correlation among these numerical predictor variables

high_val.drop('cust_id',1).corr()

We observe high correlations among many variables. Among these variables, let us see which variables have high correlation and which have least correlation.

[ ]
  1
  2
  3
  4
  5
  6
  7
  8
  9
#creating correlation matrix for the given data
corrmat = np.corrcoef(high_val.drop('cust_id',1).corr().transpose())

#Make a diagonal matrix with diagonal entry of Matrix corrmat
p = np.diagflat(corrmat.diagonal())

# subtract diagonal entries making all diagonals 0
corrmat_diag_zero = corrmat - p
print("max positive corr:",round(corrmat_diag_zero.max(),3), ", min negative corr: ", round(corrmat_diag_zero.min(),3))
max positive corr: 0.997 , min negative corr:  -0.655
9. Outlier analysis
[ ]
  1
  2
  3
  4
  5
  6
  7
  8
  9
 10
 11
 12
 13
# Outlier analysis

for i  in high_val.drop('churn',1).columns:
    plt.figure(figsize =(6,6))
    ax = sns.boxplot(data = high_val[i], orient="h", palette="Set2" )
    plt.title('Boxplot of '+' '+ i)
    plt.title("Outliers in "+' '+ i, fontsize = 14, fontweight = 'bold')
    plt.ylabel("Range", fontweight = 'bold')
    plt.xlabel(i, fontweight = 'bold')
    for patch in ax.artists:
        r, g, b, a = patch.get_facecolor()
        patch.set_facecolor((r, g, b, .3))
    plt.show()

We observe many extreme data points in our principal components. Let us treat them as these extreme values can affect our clusters.

Outlier Treatment
[ ]
  1
  2
  3
  4
# Cappign the outliers
for col in high_val.select_dtypes(exclude = 'object').columns:
    perc = high_val[col].quantile([0.05,0.95]).values
    high_val[col]= np.clip(high_val[col],perc[0],perc[1])
[ ]
  1
  2
  3
  4
  5
  6
  7
  8
  9
 10
 11
 12
 13
# Cheking whether any Outlier exists or not

for i  in high_val.drop('churn',1).select_dtypes(exclude = 'object').columns:
    plt.figure(figsize =(8,6))
    ax = sns.boxplot(high_val[i], orient="h", palette="Set2", whis = 1.5 )
    plt.title('Boxplot of '+' '+ i)
    plt.title("Outliers in "+' '+ i, fontsize = 14, fontweight = 'bold')
    plt.ylabel("Range", fontweight = 'bold')
    plt.xlabel(i, fontweight = 'bold')
    for patch in ax.artists:
        r, g, b, a = patch.get_facecolor()
        patch.set_facecolor((r, g, b, .3))
    plt.show()

We observe that after treating our dataset for outliers, only few extreme values remain. We choose to proceed with this dataset as treating them beyond this point will hamper our dataset.

9. Data Modeling and Model Evaluation
PART 1: - Non Interpretable models
A) Data Preparation
1) Copying data and changing Data type
[ ]
  1
  2
  3
  4
  5
#Replacing  original data with treated data
high_logreg = high_val.copy()
#changing churn column to numeric
high_logreg.churn= high_logreg.churn.astype("int")

[ ]
  1
high_logreg.head()

2) Creating dummies for categorical columns
[ ]
  1
  2
dummies = pd.get_dummies(high_logreg[cat_cols])
dummies.head()

[ ]
  1
dummies.shape
(30001, 24)
[ ]
  1
  2
  3
  4
  5
#Dropping all the night pack & FB user categories wtih -1.0 and age_group 1
cols_to_drop = ['night_pck_user_6_-1.0','night_pck_user_7_-1.0','night_pck_user_8_-1.0','fb_user_6_-1.0','fb_user_7_-1.0','fb_user_8_-1.0','age_group_1']

dummies.drop(cols_to_drop, axis =1, inplace = True)
dummies.head()

[ ]
  1
dummies.shape
(30001, 17)
[ ]
  1
  2
  3
#concatenating the dummy variables with Original DataFrame
high_logreg = pd.concat([high_logreg,dummies], axis =1)
high_logreg.shape
(30001, 155)
[ ]
  1
  2
  3
  4
# drop duplicate columns
high_logreg.drop(cat_cols,axis =1, inplace = True)

high_logreg.shape
(30001, 148)
3) Train Test Split
[ ]
  1
  2
  3
  4
  5
# Putting response variable to y
y = high_logreg.pop('churn')

# Putting feature variables to X
X = high_logreg.drop('cust_id',1)
[ ]
  1
X.head()

[ ]
  1
  2
print("Size of Feature variables dataset is:",X.shape)
print("Size of response variable dataset is:",y.shape)
Size of Feature variables dataset is: (30001, 146)
Size of response variable dataset is: (30001,)
[ ]
  1
  2
# Split data into train & test data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=100, stratify=y)
[ ]
  1
X_train.shape, X_test.shape, y_train.shape, y_test.shape
((21000, 146), (9001, 146), (21000,), (9001,))
4) Feature scaling
[ ]
  1
  2
print("Glimpse of Feature variables train dataset - x_train:")
X_train.head()

[ ]
  1
  2
num_var = X_train.select_dtypes(exclude='object').drop(dummies.columns,1)
num_col = num_var.columns
[ ]
  1
  2
  3
  4
  5
## Scaling the train and test data
scale = StandardScaler()
X_train[num_col] = scale.fit_transform(X_train[num_col])
X_test[num_col] = scale.fit_transform(X_test[num_col])
X_train.head()

5) Applying PCA
    In the exploratory data analaysis we saw that some columns have significant correlation among themselves. This collinearity can hamper our interpretations. Principal component analysis (PCA) is one of the most commonly used dimensionality reduction techniques in the industry. So let us adopt PCA to solve this problem. PCA will also help in dimensionality reduction.
[ ]
  1
  2
  3
#Improting the PCA module
from sklearn.decomposition import PCA
pca = PCA(svd_solver='randomized',random_state = 42)
[ ]
  1
  2
#Performing PCA on the train data
pca.fit(X_train)
PCA(copy=True, iterated_power='auto', n_components=None, random_state=42,
    svd_solver='randomized', tol=0.0, whiten=False)
[ ]
  1
  2
  3
# Let us observe pricipal components created
print("The Principal components are:")
pca.components_
The Principal components are:
array([[ 1.37875449e-01,  1.48158746e-01,  1.45639853e-01, ...,
         9.79369270e-03,  6.97338662e-03,  3.32220218e-04],
       [-1.57884707e-02, -1.75993843e-02,  1.35326112e-02, ...,
         1.35413542e-02,  1.00449679e-02,  5.52716839e-04],
       [ 1.07797002e-01,  1.47756133e-01,  1.48784019e-01, ...,
        -1.11998605e-02, -8.08486402e-03, -2.68168783e-04],
       ...,
       [-4.01884165e-16, -8.31055639e-17,  1.10347853e-16, ...,
         2.99884827e-16,  3.63173962e-16,  2.64510227e-16],
       [-0.00000000e+00, -2.77943191e-17, -5.58554684e-17, ...,
        -1.53610677e-16, -1.69551537e-16, -2.81363007e-16],
       [-0.00000000e+00, -1.64679753e-16, -1.76958415e-16, ...,
        -2.19561996e-16, -1.32895431e-16,  5.70404374e-17]])
[ ]
  1
  2
  3
# Let us also look at the variance ratio
var_ratio = np.round((pca.explained_variance_ratio_) * 100,2)
var_ratio
array([1.483e+01, 1.166e+01, 7.200e+00, 6.110e+00, 4.420e+00, 3.260e+00,
       2.930e+00, 2.790e+00, 2.430e+00, 2.370e+00, 1.990e+00, 1.800e+00,
       1.700e+00, 1.620e+00, 1.550e+00, 1.490e+00, 1.440e+00, 1.370e+00,
       1.300e+00, 1.180e+00, 1.040e+00, 9.500e-01, 8.900e-01, 8.500e-01,
       8.100e-01, 7.900e-01, 7.600e-01, 7.300e-01, 7.200e-01, 6.900e-01,
       6.600e-01, 6.500e-01, 6.200e-01, 6.000e-01, 5.600e-01, 5.500e-01,
       5.100e-01, 5.100e-01, 4.800e-01, 4.700e-01, 4.500e-01, 4.400e-01,
       4.300e-01, 4.300e-01, 4.100e-01, 4.000e-01, 3.800e-01, 3.500e-01,
       3.400e-01, 3.300e-01, 3.200e-01, 3.200e-01, 2.900e-01, 2.800e-01,
       2.800e-01, 2.700e-01, 2.500e-01, 2.400e-01, 2.300e-01, 2.300e-01,
       2.200e-01, 2.200e-01, 2.200e-01, 2.100e-01, 2.100e-01, 2.000e-01,
       2.000e-01, 2.000e-01, 1.800e-01, 1.800e-01, 1.700e-01, 1.700e-01,
       1.600e-01, 1.600e-01, 1.500e-01, 1.500e-01, 1.500e-01, 1.400e-01,
       1.400e-01, 1.300e-01, 1.100e-01, 1.100e-01, 1.100e-01, 1.000e-01,
       1.000e-01, 9.000e-02, 9.000e-02, 8.000e-02, 8.000e-02, 8.000e-02,
       8.000e-02, 8.000e-02, 8.000e-02, 7.000e-02, 7.000e-02, 7.000e-02,
       6.000e-02, 6.000e-02, 6.000e-02, 6.000e-02, 5.000e-02, 5.000e-02,
       5.000e-02, 4.000e-02, 4.000e-02, 4.000e-02, 4.000e-02, 3.000e-02,
       3.000e-02, 3.000e-02, 3.000e-02, 3.000e-02, 3.000e-02, 2.000e-02,
       2.000e-02, 2.000e-02, 2.000e-02, 2.000e-02, 2.000e-02, 2.000e-02,
       2.000e-02, 2.000e-02, 2.000e-02, 2.000e-02, 1.000e-02, 1.000e-02,
       1.000e-02, 1.000e-02, 1.000e-02, 1.000e-02, 1.000e-02, 1.000e-02,
       1.000e-02, 1.000e-02, 1.000e-02, 1.000e-02, 0.000e+00, 0.000e+00,
       0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,
       0.000e+00, 0.000e+00])
[ ]
  1
  2
var_cum = np.cumsum(var_ratio)
var_cum
array([14.83, 26.49, 33.69, 39.8 , 44.22, 47.48, 50.41, 53.2 , 55.63,
       58.  , 59.99, 61.79, 63.49, 65.11, 66.66, 68.15, 69.59, 70.96,
       72.26, 73.44, 74.48, 75.43, 76.32, 77.17, 77.98, 78.77, 79.53,
       80.26, 80.98, 81.67, 82.33, 82.98, 83.6 , 84.2 , 84.76, 85.31,
       85.82, 86.33, 86.81, 87.28, 87.73, 88.17, 88.6 , 89.03, 89.44,
       89.84, 90.22, 90.57, 90.91, 91.24, 91.56, 91.88, 92.17, 92.45,
       92.73, 93.  , 93.25, 93.49, 93.72, 93.95, 94.17, 94.39, 94.61,
       94.82, 95.03, 95.23, 95.43, 95.63, 95.81, 95.99, 96.16, 96.33,
       96.49, 96.65, 96.8 , 96.95, 97.1 , 97.24, 97.38, 97.51, 97.62,
       97.73, 97.84, 97.94, 98.04, 98.13, 98.22, 98.3 , 98.38, 98.46,
       98.54, 98.62, 98.7 , 98.77, 98.84, 98.91, 98.97, 99.03, 99.09,
       99.15, 99.2 , 99.25, 99.3 , 99.34, 99.38, 99.42, 99.46, 99.49,
       99.52, 99.55, 99.58, 99.61, 99.64, 99.66, 99.68, 99.7 , 99.72,
       99.74, 99.76, 99.78, 99.8 , 99.82, 99.84, 99.86, 99.87, 99.88,
       99.89, 99.9 , 99.91, 99.92, 99.93, 99.94, 99.95, 99.96, 99.97,
       99.98, 99.98, 99.98, 99.98, 99.98, 99.98, 99.98, 99.98, 99.98,
       99.98, 99.98])
Making a scree plot for the explained variance
[ ]
  1
  2
  3
  4
  5
  6
  7
fig = plt.figure(figsize=[12,8])
plt.vlines(x=60, ymax=100, ymin=0, colors="r", linestyles="--")
plt.hlines(y=94, xmax=150, xmin=0, colors="g", linestyles="--")
plt.plot(var_cum)
plt.xlabel('Number of PCA components')
plt.ylabel("Cumulative variance explained")
plt.show()

Looks like 60 components are enough to describe 94% of the variance in the dataset. We'll choose 60 components for our modeling

6) Adopting Incremental PCA
Let us Build the dataframe using Incremental PCA for better efficiency.

[ ]
  1
  2
  3
  4
from sklearn.decomposition import IncrementalPCA
pca_final = IncrementalPCA(n_components = 60)

X_train_pca = pca_final.fit_transform(X_train)
[ ]
  1
  2
print("Size of earlier dataset was :",X_train.shape)
print("Size of dataset after PCA is:", X_train_pca.shape)
Size of earlier dataset was : (21000, 146)
Size of dataset after PCA is: (21000, 60)
[ ]
  1
  2
  3
  4
  5
  6
  7
  8
  9
#creating correlation matrix for the given data
corrmat = np.corrcoef(X_train_pca.transpose())

#Make a diagonal matrix with diagonal entry of Matrix corrmat
p = np.diagflat(corrmat.diagonal())

# subtract diagonal entries making all diagonals 0
corrmat_diag_zero = corrmat - p
print("max positive corr:",round(corrmat_diag_zero.max(),3), ", min negative corr: ", round(corrmat_diag_zero.min(),3))
max positive corr: 0.004 , min negative corr:  -0.005
We can see from the above calculations that the correlation among the attributes is almost 0, we can proceed with these principal components.

[ ]
  1
  2
X_test_pca = pca_final.transform(X_test)
X_test_pca.shape
(9001, 60)
7) Model Building
Let us now build various models on this PCA Transformed dataset to predict churn

Model1- PCA and Logistic Regression
[ ]
  1
  2
  3
  4
  5
from sklearn.linear_model import LogisticRegression

# Make an instance of the Model.
# default solver is very slow so changed to 'lbfgs'
lr = LogisticRegression(solver = 'lbfgs',class_weight="balanced")
[ ]
  1
X_train_pca.shape, y_train.shape
((21000, 60), (21000,))
[ ]
  1
  2
# Training the model on the data
lr.fit(X_train_pca, y_train)
LogisticRegression(C=1.0, class_weight='balanced', dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
[ ]
  1
  2
  3
  4
  5
  6
  7
  8
  9
 10
 11
 12
 13
 14
 15
 16
 17
#prediction on test data
y_pred = lr.predict(X_test_pca)

#create confusion matrix
cm = confusion_matrix(y_test,y_pred)
print("confusoin matrix \t\n",cm)

#checking sesitivity 
print("sensitivity \t", (cm[1,1]/(cm[1,0]+cm[1,1])).round(2))


#checking  specificity
print("specificity \t", (cm[0,0]/(cm[0,0]+cm[0,1])).round(2))

#check area under the curve
from sklearn.metrics import roc_auc_score
print("area under the curve \t",round(roc_auc_score(y_test,y_pred),2))
confusoin matrix 	
 [[6906 1363]
 [ 126  606]]
sensitivity 	 0.83
specificity 	 0.84
area under the curve 	 0.83
Hyper Parameter tuning - Logistic Regression
[ ]
  1
  2
  3
  4
  5
  6
  7
  8
  9
 10
 11
 12
 13
 14
 15
 16
 17
logistic = LogisticRegression(solver = 'lbfgs',class_weight= "balanced")
penalty = ['l1', 'l2','elasticnet']
C = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]

param_grid = dict(penalty=penalty,
                  C=C)

# #creating 5 folds
folds = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 4) 

#GradientSearchCV
logreg_model =  GridSearchCV(estimator=logistic,
                    param_grid=param_grid,
                    scoring='roc_auc',
                    cv = folds,
                    verbose=1,
                    n_jobs=-1)
[ ]
  1
  2
#model fitting
logreg_model.fit(X_train_pca,y_train)
Fitting 5 folds for each of 24 candidates, totalling 120 fits
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.
[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    8.5s
[Parallel(n_jobs=-1)]: Done 120 out of 120 | elapsed:   13.1s finished
GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=4, shuffle=True),
             error_score=nan,
             estimator=LogisticRegression(C=1.0, class_weight='balanced',
                                          dual=False, fit_intercept=True,
                                          intercept_scaling=1, l1_ratio=None,
                                          max_iter=100, multi_class='auto',
                                          n_jobs=None, penalty='l2',
                                          random_state=None, solver='lbfgs',
                                          tol=0.0001, verbose=0,
                                          warm_start=False),
             iid='deprecated', n_jobs=-1,
             param_grid={'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000],
                         'penalty': ['l1', 'l2', 'elasticnet']},
             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,
             scoring='roc_auc', verbose=1)
[ ]
  1
  2
# cross validation results
pd.DataFrame(logreg_model.cv_results_).head()

[ ]
  1
  2
  3
# print best hyperparameters
print("Best AUC: ", logreg_model.best_score_)
print("Best hyperparameters: ", logreg_model.best_params_)
Best AUC:  0.9000371296014563
Best hyperparameters:  {'C': 0.01, 'penalty': 'l2'}
[ ]
  1
  2
  3
#fitting the model with best parameters
logistic = LogisticRegression(solver = 'lbfgs',class_weight= "balanced", C = logreg_model.best_params_['C'],penalty=logreg_model.best_params_['penalty'])
logreg_model.fit(X_train_pca, y_train)
Fitting 5 folds for each of 24 candidates, totalling 120 fits
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.
[Parallel(n_jobs=-1)]: Done  76 tasks      | elapsed:    3.7s
[Parallel(n_jobs=-1)]: Done 120 out of 120 | elapsed:    6.1s finished
GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=4, shuffle=True),
             error_score=nan,
             estimator=LogisticRegression(C=1.0, class_weight='balanced',
                                          dual=False, fit_intercept=True,
                                          intercept_scaling=1, l1_ratio=None,
                                          max_iter=100, multi_class='auto',
                                          n_jobs=None, penalty='l2',
                                          random_state=None, solver='lbfgs',
                                          tol=0.0001, verbose=0,
                                          warm_start=False),
             iid='deprecated', n_jobs=-1,
             param_grid={'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000],
                         'penalty': ['l1', 'l2', 'elasticnet']},
             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,
             scoring='roc_auc', verbose=1)
[ ]
  1
  2
  3
  4
  5
  6
  7
  8
  9
 10
 11
 12
 13
 14
 15
 16
 17
#prediction on test data
y_pred = logreg_model.predict(X_test_pca)

#create confusion matrix
cm = confusion_matrix(y_test,y_pred)
print("confusoin matrix \t\n",cm)

#checking sesitivity 
print("sensitivity \t", (cm[1,1]/(cm[1,0]+cm[1,1])).round(2))


#checking  specificity
print("specificity \t", (cm[0,0]/(cm[0,0]+cm[0,1])).round(2))

# check area under curve
y_pred_prob = logreg_model.predict_proba(X_test_pca)[:, 1]
print("AUC:    \t", round(roc_auc_score(y_test, y_pred_prob),2))
confusoin matrix 	
 [[6900 1369]
 [ 126  606]]
sensitivity 	 0.83
specificity 	 0.83
AUC:    	 0.9
Model2- PCA and SVM
[ ]
  1
X_train_pca.shape, y_train.shape
((21000, 60), (21000,))
[ ]
  1
  2
  3
  4
  5
  6
  7
# linear model

model_linear = SVC(kernel='linear', class_weight="balanced")
model_linear.fit(X_train_pca, y_train)

# predict
y_pred = model_linear.predict(X_test_pca)
[ ]
  1
  2
  3
  4
  5
  6
  7
# confusion matrix and accuracy

# accuracy
print("accuracy:", accuracy_score(y_true=y_test, y_pred=y_pred), "\n")

# cm
print(confusion_matrix(y_true=y_test, y_pred=y_pred))
accuracy: 0.8471280968781246 

[[7027 1242]
 [ 134  598]]
[ ]
  1
  2
  3
  4
  5
  6
  7
  8
  9
 10
 11
# non-linear model
# using rbf kernel, C=1, default value of gamma

# model
non_linear_model = SVC(kernel='rbf',class_weight="balanced")

# fit
non_linear_model.fit(X_train_pca, y_train)

# predict
y_pred = non_linear_model.predict(X_test_pca)
[ ]
  1
  2
  3
  4
  5
  6
  7
# confusion matrix and accuracy

# accuracy
print("accuracy:", accuracy_score(y_true=y_test, y_pred=y_pred), "\n")

# cm
print(confusion_matrix(y_true=y_test, y_pred=y_pred))
accuracy: 0.8879013442950783 

[[7421  848]
 [ 161  571]]
Hyper Parameter tuning for SVC
Note: The Below GridSearch Step will take almost 40-45 mins to run.
[ ]
  1
  2
  3
  4
  5
  6
  7
  8
  9
 10
 11
 12
 13
 14
 15
 16
 17
 18
 19
 20
 21
 22
# creating a KFold object with 5 splits 
folds = StratifiedKFold(n_splits = 3, shuffle = True, random_state = 100) 

# specify range of hyperparameters
# Set the parameters by cross-validation
hyper_params = [ {'gamma': [1e-1, 1e-2],
                     'C': [1, 10]}]


# specify model
model = SVC(class_weight= "balanced",random_state = 100, kernel = 'linear')

# set up GridSearchCV()
model_cv =  GridSearchCV(estimator=model,
                    param_grid=hyper_params,
                         cv = folds,
                    refit = True, verbose = 3,return_train_score=True, n_jobs= -1)   

# fit the model
model_cv.fit(X_train_pca, y_train)                  
               

Fitting 3 folds for each of 4 candidates, totalling 12 fits
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.
[Parallel(n_jobs=-1)]: Done  10 out of  12 | elapsed: 19.0min remaining:  3.8min
[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed: 28.2min finished
GridSearchCV(cv=StratifiedKFold(n_splits=3, random_state=100, shuffle=True),
             error_score=nan,
             estimator=SVC(C=1.0, break_ties=False, cache_size=200,
                           class_weight='balanced', coef0=0.0,
                           decision_function_shape='ovr', degree=3,
                           gamma='scale', kernel='linear', max_iter=-1,
                           probability=False, random_state=100, shrinking=True,
                           tol=0.001, verbose=False),
             iid='deprecated', n_jobs=-1,
             param_grid=[{'C': [1, 10], 'gamma': [0.1, 0.01]}],
             pre_dispatch='2*n_jobs', refit=True, return_train_score=True,
             scoring=None, verbose=3)
[ ]
  1
  2
  3
# cv results
cv_results = pd.DataFrame(model_cv.cv_results_)
cv_results

[ ]
  1
  2
  3
  4
  5
  6
  7
  8
  9
 10
 11
 12
 13
# plotting 'C'

plt.figure()
plt.plot(model_cv.cv_results_["param_C"], 
model_cv.cv_results_["mean_train_score"], 
label="training accuracy")
plt.plot(model_cv.cv_results_["param_C"], 
model_cv.cv_results_["mean_test_score"], 
label="test accuracy")
plt.xlabel('C')
plt.ylabel("f1")
plt.legend()
plt.show()

[ ]
  1
  2
  3
  4
  5
# printing the optimal accuracy score and hyperparameters
best_score = model_cv.best_score_
best_hyperparams = model_cv.best_params_

print("The best test score is {0} corresponding to hyperparameters {1}".format(best_score, best_hyperparams))
The best test score is 0.8495238095238095 corresponding to hyperparameters {'C': 1, 'gamma': 0.1}
Building and Evaluating the Final Model
[ ]
  1
  2
  3
  4
  5
  6
  7
  8
  9
 10
 11
 12
 13
# model with optimal hyperparameters

# model
model = SVC(C=1, gamma=0.01, class_weight= 'balanced',random_state=100,kernel = 'linear',verbose = 1)

model.fit(X_train_pca, y_train)
y_pred = model.predict(X_test_pca)

# metrics
print("accuracy", accuracy_score(y_test, y_pred), "\n")
print(confusion_matrix(y_test, y_pred), "\n")
print(classification_report(y_test, y_pred), "\n")

[LibSVM]accuracy 0.8471280968781246 

[[7027 1242]
 [ 134  598]] 

              precision    recall  f1-score   support

           0       0.98      0.85      0.91      8269
           1       0.33      0.82      0.47       732

    accuracy                           0.85      9001
   macro avg       0.65      0.83      0.69      9001
weighted avg       0.93      0.85      0.87      9001
 

[ ]
  1
  2
  3
  4
  5
  6
  7
  8
  9
 10
 11
 12
 13
 14
#create confusion matrix
cm = confusion_matrix(y_test,y_pred)
print("confusoin matrix \t\n",cm)

#checking sesitivity 
print("sensitivity \t", (cm[1,1]/(cm[1,0]+cm[1,1])).round(2))


#checking  specificity
print("specificity \t", (cm[0,0]/(cm[0,0]+cm[0,1])).round(2))

# check area under curve
y_pred_prob = logreg_model.predict_proba(X_test_pca)[:, 1]
print("AUC:    \t", round(roc_auc_score(y_test, y_pred_prob),2)) 
confusoin matrix 	
 [[7027 1242]
 [ 134  598]]
sensitivity 	 0.82
specificity 	 0.85
AUC:    	 0.9
Model3 PCA and Random Forest
Default Hyperparameters
Let's first fit a random forest model with default hyperparameters.

[ ]
  1
  2
  3
  4
  5
# Importing random forest classifier from sklearn library
from sklearn.ensemble import RandomForestClassifier

# Running the random forest with default parameters.
rfc = RandomForestClassifier()
[ ]
  1
  2
# fit
rfc.fit(X_train_pca,y_train)
RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_impurity_split=None,
                       min_samples_leaf=1, min_samples_split=2,
                       min_weight_fraction_leaf=0.0, n_estimators=100,
                       n_jobs=None, oob_score=False, random_state=None,
                       verbose=0, warm_start=False)
[ ]
  1
  2
# Making predictions
predictions = rfc.predict(X_test_pca)
[ ]
  1
  2
# Let's check the report of our default model
print(classification_report(y_test,predictions))
              precision    recall  f1-score   support

           0       0.93      0.99      0.96      8269
           1       0.72      0.18      0.29       732

    accuracy                           0.93      9001
   macro avg       0.83      0.59      0.63      9001
weighted avg       0.92      0.93      0.91      9001

[ ]
  1
  2
# Printing confusion matrix
print(confusion_matrix(y_test,predictions))
[[8218   51]
 [ 599  133]]
[ ]
  1
print(accuracy_score(y_test,predictions))
0.9277858015776025
So far so good, let's now look at the list of hyperparameters which we can tune to improve model performance.

Hyperparameter Tuning
Tuning max_depth
Let's try to find the optimum values for max_depth and understand how the value of max_depth impacts the overall accuracy of the ensemble.

[ ]
  1
  2
  3
  4
  5
  6
  7
  8
  9
 10
 11
 12
 13
 14
 15
 16
 17
# specify number of folds for k-fold CV
n_folds = 5

# parameters to build the model on
parameters = {'max_depth': range(2, 20, 5)}

# instantiate the model
rf = RandomForestClassifier(class_weight= 'balanced', random_state=100)


# fit tree on training data
rf = GridSearchCV(rf, parameters, cv=n_folds, 
                   scoring="accuracy",verbose =1, n_jobs = -1, return_train_score=True)
rf.fit(X_train_pca, y_train)

print(rf.best_score_)
print(rf.best_params_)
Fitting 5 folds for each of 4 candidates, totalling 20 fits
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.
[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:  1.4min finished
0.9251904761904761
{'max_depth': 17}
[ ]
  1
  2
  3
# scores of GridSearch CV
scores = rf.cv_results_
pd.DataFrame(scores).head()

[ ]
  1
  2
  3
  4
  5
  6
  7
  8
  9
 10
 11
 12
 13
# plotting accuracies with max_depth
plt.figure()
plt.plot(scores["param_max_depth"], 
         scores["mean_train_score"], 
         label="training accuracy")
plt.plot(scores["param_max_depth"], 
         scores["mean_test_score"], 
         label="test accuracy")
plt.xlabel("max_depth")
plt.ylabel("Accuracy")
plt.legend()
plt.show()


You can see that as we increase the value of max_depth, both train and test scores increase till a point, but after that test score starts to decrease. The ensemble tries to overfit as we increase the max_depth.

Thus, controlling the depth of the constituent trees will help reduce overfitting in the forest.

Tuning n_estimators
Let's try to find the optimum values for n_estimators and understand how the value of n_estimators impacts the overall accuracy. Notice that we'll specify an appropriately low value of max_depth, so that the trees do not overfit.

[ ]
  1
  2
  3
  4
  5
  6
  7
  8
  9
 10
 11
 12
 13
 14
 15
 16
 17
 18
# specify number of folds for k-fold CV
n_folds = 5

# parameters to build the model on
parameters = {'n_estimators': range(100, 1500, 500)}

# instantiate the model (note we are specifying a max_depth)
rf = RandomForestClassifier(max_depth=4, class_weight ='balanced', random_state=100)


# fit tree on training data
rf = GridSearchCV(rf, parameters, 
                    cv=n_folds, verbose = 1,
                   scoring="accuracy",return_train_score=True, n_jobs = -1)
rf.fit(X_train_pca, y_train)

print(rf.best_score_)
print(rf.best_params_)
Fitting 5 folds for each of 3 candidates, totalling 15 fits
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.
[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:  5.3min finished
0.8613809523809524
{'n_estimators': 1100}
[ ]
  1
  2
  3
# scores of GridSearch CV
scores = rf.cv_results_
pd.DataFrame(scores).head()

[ ]
  1
  2
  3
  4
  5
  6
  7
  8
  9
 10
 11
 12
 13
# plotting accuracies with n_estimators
plt.figure()
plt.plot(scores["param_n_estimators"], 
         scores["mean_train_score"], 
         label="training accuracy")
plt.plot(scores["param_n_estimators"], 
         scores["mean_test_score"], 
         label="test accuracy")
plt.xlabel("n_estimators")
plt.ylabel("Accuracy")
plt.legend()
plt.show()


Tuning max_features
Let's see how the model performance varies with max_features, which is the maximum numbre of features considered for splitting at a node.

[ ]
  1
  2
  3
  4
  5
  6
  7
  8
  9
 10
 11
 12
 13
 14
 15
 16
 17
 18

# specify number of folds for k-fold CV
n_folds = 5

# parameters to build the model on
parameters = {'max_features': [4, 8, 14, 20, 24]}

# instantiate the model
rf = RandomForestClassifier(max_depth=4,class_weight='balanced',random_state=100)


# fit tree on training data
rf = GridSearchCV(rf, parameters, verbose = 1,cv=n_folds, 
                   scoring="accuracy",n_jobs = -1, return_train_score = True)
rf.fit(X_train_pca, y_train)

print(rf.best_score_)
print(rf.best_params_)
Fitting 5 folds for each of 5 candidates, totalling 25 fits
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.
[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:  2.1min finished
0.8619523809523809
{'max_features': 4}
[ ]
  1
  2
  3
# scores of GridSearch CV
scores = rf.cv_results_
pd.DataFrame(scores).head()

[ ]
  1
  2
  3
  4
  5
  6
  7
  8
  9
 10
 11
 12
 13
# plotting accuracies with max_features
plt.figure()
plt.plot(scores["param_max_features"], 
         scores["mean_train_score"], 
         label="training accuracy")
plt.plot(scores["param_max_features"], 
         scores["mean_test_score"], 
         label="test accuracy")
plt.xlabel("max_features")
plt.ylabel("Accuracy")
plt.legend()
plt.show()


The training and test scores both seem to increase as we increase max_features, and the model doesn't seem to overfit more with increasing max_features.

Tuning min_samples_leaf
The hyperparameter min_samples_leaf is the minimum number of samples required to be at a leaf node:

If int, then consider min_samples_leaf as the minimum number.
If float, then min_samples_leaf is a percentage and ceil(min_samples_leaf * n_samples) are the minimum number of samples for each node.
Let's now check the optimum value for min samples leaf in our case.

[ ]
  1
  2
  3
  4
  5
  6
  7
  8
  9
 10
 11
 12
 13
 14
 15
 16
 17
 18

# specify number of folds for k-fold CV
n_folds = 5

# parameters to build the model on
parameters = {'min_samples_leaf': range(100, 400, 50)}

# instantiate the model
rf = RandomForestClassifier(class_weight ='balanced',random_state=100)


# fit tree on training data
rf = GridSearchCV(rf, parameters, 
                    cv=n_folds, 
                   scoring="accuracy",verbose =1, return_train_score = True, n_jobs = -1)
rf.fit(X_train_pca, y_train)
print(rf.best_score_)
print(rf.best_params_)
Fitting 5 folds for each of 6 candidates, totalling 30 fits
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.
[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:  2.4min finished
0.8762857142857141
{'min_samples_leaf': 100}
[ ]
  1
  2
  3
# scores of GridSearch CV
scores = rf.cv_results_
pd.DataFrame(scores).head()

[ ]
  1
  2
  3
  4
  5
  6
  7
  8
  9
 10
 11
 12
# plotting accuracies with min_samples_leaf
plt.figure()
plt.plot(scores["param_min_samples_leaf"], 
         scores["mean_train_score"], 
         label="training accuracy")
plt.plot(scores["param_min_samples_leaf"], 
         scores["mean_test_score"], 
         label="test accuracy")
plt.xlabel("min_samples_leaf")
plt.ylabel("Accuracy")
plt.legend()
plt.show()

We can see that the model starts of overfit as you decrease the value of min_samples_leaf.

Tuning min_samples_split
Let's now look at the performance of the ensemble as we vary min_samples_split.

[ ]
  1
  2
  3
  4
  5
  6
  7
  8
  9
 10
 11
 12
 13
 14
 15
 16
 17
 18
 19

# specify number of folds for k-fold CV
n_folds = 5

# parameters to build the model on
parameters = {'min_samples_split': range(200, 500, 50)}

# instantiate the model
rf = RandomForestClassifier(class_weight = 'balanced',random_state=100)


# fit tree on training data
rf = GridSearchCV(rf, parameters, verbose = 1,
                    cv=n_folds, 
                   scoring="accuracy",n_jobs =-1, return_train_score = True)
rf.fit(X_train_pca, y_train)

print(rf.best_score_)
print(rf.best_params_)
Fitting 5 folds for each of 6 candidates, totalling 30 fits
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.
[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:  2.3min finished
0.900952380952381
{'min_samples_split': 200}
[ ]
  1
  2
  3
# scores of GridSearch CV
scores = rf.cv_results_
pd.DataFrame(scores).head()

[ ]
  1
  2
  3
  4
  5
  6
  7
  8
  9
 10
 11
 12
# plotting accuracies with min_samples_split
plt.figure()
plt.plot(scores["param_min_samples_split"], 
         scores["mean_train_score"], 
         label="training accuracy")
plt.plot(scores["param_min_samples_split"], 
         scores["mean_test_score"], 
         label="test accuracy")
plt.xlabel("min_samples_split")
plt.ylabel("Accuracy")
plt.legend()
plt.show()

Grid Search to Find Optimal Hyperparameters
We can now find the optimal hyperparameters using GridSearchCV.

Note: The Below GridSearch Step will take almost 30-35 mins to run.
[ ]
  1
  2
  3
  4
  5
  6
  7
  8
  9
 10
 11
 12
# Create the parameter grid based on the results of random search  
param_grid = {
    'max_depth': [4,8,10],
    'min_samples_leaf': range(100, 400, 200),
    'min_samples_split': range(200, 500, 200),
    'n_estimators': [100,200, 300], 
    'max_features': [5, 10]}
# Create a based model
rf = RandomForestClassifier()
# Instantiate the grid search model
grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, 
                          cv = 3, n_jobs = -1,verbose = 1)
[ ]
  1
  2
  3
# Fit the grid search to the data
grid_search.fit(X_train_pca, y_train)

Fitting 3 folds for each of 72 candidates, totalling 216 fits
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.
[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  3.2min
[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed: 20.0min
[Parallel(n_jobs=-1)]: Done 216 out of 216 | elapsed: 23.8min finished
GridSearchCV(cv=3, error_score=nan,
             estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                                              class_weight=None,
                                              criterion='gini', max_depth=None,
                                              max_features='auto',
                                              max_leaf_nodes=None,
                                              max_samples=None,
                                              min_impurity_decrease=0.0,
                                              min_impurity_split=None,
                                              min_samples_leaf=1,
                                              min_samples_split=2,
                                              min_weight_fraction_leaf=0.0,
                                              n_estimators=100, n_jobs=None,
                                              oob_score=False,
                                              random_state=None, verbose=0,
                                              warm_start=False),
             iid='deprecated', n_jobs=-1,
             param_grid={'max_depth': [4, 8, 10], 'max_features': [5, 10],
                         'min_samples_leaf': range(100, 400, 200),
                         'min_samples_split': range(200, 500, 200),
                         'n_estimators': [100, 200, 300]},
             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,
             scoring=None, verbose=1)
[ ]
  1
  2
# printing the optimal accuracy score and hyperparameters
print('We can get accuracy of',grid_search.best_score_,'using',grid_search.best_params_)
We can get accuracy of 0.9193333333333333 using {'max_depth': 10, 'max_features': 10, 'min_samples_leaf': 100, 'min_samples_split': 200, 'n_estimators': 100}
[ ]
  1
  2
  3
  4
  5
  6
# model with the best hyperparameters
from sklearn.ensemble import RandomForestClassifier
rfc = RandomForestClassifier(bootstrap=True,class_weight = 'balanced',
                             max_depth=grid_search.best_params_['max_depth'],
                             min_samples_split=grid_search.best_params_['min_samples_split'],
                             max_features=grid_search.best_params_['max_features'],n_estimators =grid_search.best_params_['n_estimators'], random_state=100)
[ ]
  1
  2
# fit
rfc.fit(X_train_pca,y_train)
RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',
                       criterion='gini', max_depth=10, max_features=10,
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_impurity_split=None,
                       min_samples_leaf=1, min_samples_split=200,
                       min_weight_fraction_leaf=0.0, n_estimators=100,
                       n_jobs=None, oob_score=False, random_state=100,
                       verbose=0, warm_start=False)
[ ]
  1
  2
# predict
predictions = rfc.predict(X_test_pca)
[ ]
  1
print(classification_report(y_test,predictions))
              precision    recall  f1-score   support

           0       0.97      0.90      0.93      8269
           1       0.39      0.73      0.50       732

    accuracy                           0.88      9001
   macro avg       0.68      0.81      0.72      9001
weighted avg       0.93      0.88      0.90      9001

[ ]
  1
  2
  3
  4
  5
  6
  7
  8
  9
 10
 11
 12
 13
 14
#create confusion matrix
cm = confusion_matrix(y_test,predictions)
print("confusoin matrix \t\n",cm)

#checking sesitivity 
print("sensitivity \t", (cm[1,1]/(cm[1,0]+cm[1,1])).round(2))


#checking  specificity
print("specificity \t", (cm[0,0]/(cm[0,0]+cm[0,1])).round(2))

# check area under curve
y_pred_prob = rfc.predict_proba(X_test_pca)[:, 1]
print("AUC:    \t", round(roc_auc_score(y_test, y_pred_prob),2)) 
confusoin matrix 	
 [[7423  846]
 [ 199  533]]
sensitivity 	 0.73
specificity 	 0.9
AUC:    	 0.9
8) Final Choice of Model
Recall is the most important business metric for the telecom churn problem. The company would like to identify most customers at risk of churning, even if there are many customers that are misclassified as churn. The cost to the company of churning is much higher than having a few false positives.

Model/Metricity 
a) Logistic Regression
- Sensitivity/Recall  - .83
- Specificity         - .83
- Roc AUC Score       - .90 
b) SMV
- Sensitivity/Recall  - .82
- Specificity         - .85
- Roc AUC Score       - .90 
c) Random Forest
- Sensitivity/Recall  - .73
- Specificity         - .90
- Roc AUC Score       - .90    
INFERENCE:

We choose logistic regression from all the above models as it has less time complexity and take less memory compared to all above models. Morevover, its sensitivity is very good which is our prime requirement in this case study.

PART 2: - Interpretable models
Let us now build models to identify the churn indicators. Identification of these churn indicators will help in retaining customers that are likely to churn.

[ ]
  1
  2
  3
# create a copy of dataset
high_df = high_val.copy()
high_df.head()

1) Data Preparation
a) Let us drop highly correlated features
a) 2G data recharge in the month of june

[ ]
  1
  2
  3
col_2g_6 = ['max_rech_data_6','count_rech_2g_6','vol_2g_mb_6','monthly_2g_6','sachet_2g_6']
plt.figure(figsize=[8,6])
sns.heatmap(high_df[col_2g_6].corr(), annot=True,cmap="BrBG" , robust=True,linewidth=0.1, vmin=-1)

We observe that count_rech_2g_6 has high correlation with other features. Let us drop count_rech_2g_6 to avoid multicollinearity.

[ ]
  1
high_df.drop('count_rech_2g_6',1,inplace = True)
b) 2G data recharge in the month of july

[ ]
  1
  2
  3
col_2g_7 = ['max_rech_data_7','count_rech_2g_7','vol_2g_mb_7','monthly_2g_7','sachet_2g_7']
plt.figure(figsize=[8,6])
sns.heatmap(high_df[col_2g_7].corr(), annot=True,cmap="BrBG" , robust=True,linewidth=0.1, vmin=-1)

We observe that count_rech_2g_7 has high correlation with other features. Let us drop count_rech_2g_6 to avoid multicollinearity.

[ ]
  1
high_df.drop('count_rech_2g_7',1,inplace = True)
c) 2G data recharge in the month of August

[ ]
  1
  2
  3
col_2g_8 = ['max_rech_data_8','count_rech_2g_8','vol_2g_mb_8','monthly_2g_8','sachet_2g_8']
plt.figure(figsize=[8,6])
sns.heatmap(high_df[col_2g_8].corr(), annot=True,cmap="BrBG" , robust=True,linewidth=0.1, vmin=-1)

We observe that count_rech_2g_8 has high correlation with other features. Let us drop count_rech_2g_6 to avoid multicollinearity.

[ ]
  1
high_df.drop('count_rech_2g_8',1,inplace = True)
d) 3G data recharge in the month of june

[ ]
  1
  2
  3
col_3g_6 = ['max_rech_data_6','count_rech_3g_6','vol_3g_mb_6','monthly_3g_6','sachet_3g_6']
plt.figure(figsize=[8,6])
sns.heatmap(high_df[col_3g_6].corr(), annot=True,cmap="BrBG" , robust=True,linewidth=0.1, vmin=-1)

We observe that count_rech_3g_6 has high correlation with other features. Let us drop count_rech_2g_6 to avoid multicollinearity.

[ ]
  1
high_df.drop('count_rech_3g_6',1,inplace = True)
e) 3G data recharge in the month of july

[ ]
  1
  2
  3
col_3g_7 = ['max_rech_data_7','count_rech_3g_7','vol_3g_mb_7','monthly_3g_7','sachet_3g_7']
plt.figure(figsize=[8,6])
sns.heatmap(high_df[col_3g_7].corr(), annot=True,cmap="BrBG" , robust=True,linewidth=0.1, vmin=-1)

We observe that count_rech_3g_7 has high correlation with other features. Let us drop count_rech_2g_6 to avoid multicollinearity.

[ ]
  1
high_df.drop('count_rech_3g_7',1,inplace = True)
f) 3G data recharge in the month of August

[ ]
  1
  2
  3
col_3g_8 = ['max_rech_data_8','count_rech_3g_8','vol_3g_mb_8','monthly_3g_8','sachet_3g_8']
plt.figure(figsize=[8,6])
sns.heatmap(high_df[col_3g_8].corr(), annot=True,cmap="BrBG" , robust=True,linewidth=0.1, vmin=-1)

We observe that count_rech_3g_8 has high correlation with other features. Let us drop count_rech_2g_6 to avoid multicollinearity.

[ ]
  1
high_df.drop('count_rech_3g_8',1,inplace = True)
g) Total outgoing calls made in the month of june

[ ]
  1
  2
  3
og_col_6 = ['loc_og_mou_6','std_og_mou_6','spl_og_mou_6','total_og_mou_6','std_og_t2t_mou_6','std_og_t2m_mou_6','std_og_t2f_mou_6','loc_og_t2t_mou_6','loc_og_t2m_mou_6','loc_og_t2f_mou_6','onnet_mou_6','offnet_mou_6']
plt.figure(figsize=[8,6])
sns.heatmap(high_df[og_col_6].corr(), annot=True,cmap="BrBG" , robust=True,linewidth=0.1, vmin=-1)

We observe that loc_og_mou_6, std_og_mou_6 tot, l_og_mou_6, onnet_7 & offnet_6 have strong correlation with other features. These features should be taken care of to handle multicollinearity.

[ ]
  1
high_df.drop(['loc_og_mou_6', 'std_og_mou_6','total_og_mou_6','offnet_mou_6','onnet_mou_6'],1,inplace=True)
h) Total outgoing calls made in the month of july

[ ]
  1
  2
  3
og_col_7 = ['loc_og_mou_7','std_og_mou_7','spl_og_mou_7','total_og_mou_7','std_og_t2t_mou_7','std_og_t2m_mou_7','std_og_t2f_mou_7','loc_og_t2t_mou_7','loc_og_t2m_mou_7','loc_og_t2f_mou_7','loc_og_mou_7','onnet_mou_7','offnet_mou_7']
plt.figure(figsize=[8,6])
sns.heatmap(high_df[og_col_7].corr(), annot=True,cmap="BrBG" , robust=True,linewidth=0.1, vmin=-1)

We observe that loc_og_mou_7, std_og_mou_7 and total_og_mou_7 have strong correlation with other features. These features should be taken care of to handle multicollinearity.

[ ]
  1
high_df.drop(['loc_og_mou_7', 'std_og_mou_7','total_og_mou_7','onnet_mou_7','offnet_mou_7'],1,inplace=True)
i) Total outgoing calls made in the month of August

[ ]
  1
  2
  3
og_col_8 = ['loc_og_mou_8','std_og_mou_8','spl_og_mou_8','total_og_mou_8','std_og_t2t_mou_8','std_og_t2m_mou_8','std_og_t2f_mou_8','loc_og_t2t_mou_8','loc_og_t2m_mou_8','loc_og_t2f_mou_8','loc_og_mou_8','onnet_mou_8','offnet_mou_8']
plt.figure(figsize=[8,6])
sns.heatmap(high_df[og_col_8].corr(), annot=True,cmap="BrBG" , robust=True,linewidth=0.1, vmin=-1)

We observe that loc_og_mou_8, std_og_mou_8 and total_og_mou_8 have strong correlation with other features. These features should be taken care of to handle multicollinearity.

[ ]
  1
high_df.drop(['loc_og_mou_8', 'std_og_mou_8','total_og_mou_8','onnet_mou_8','offnet_mou_8'],1,inplace=True)
j) Total incoming calls made in the month of june

[ ]
  1
  2
  3
ic_col_6 = ['loc_ic_mou_6','std_ic_mou_6','isd_ic_mou_6','total_ic_mou_6','std_ic_t2t_mou_6','std_ic_t2m_mou_6','std_ic_t2f_mou_6','loc_ic_t2t_mou_6','loc_ic_t2m_mou_6','loc_ic_t2f_mou_6']
plt.figure(figsize=[8,6])
sns.heatmap(high_df[ic_col_6].corr(), annot=True,cmap="BrBG" , robust=True,linewidth=0.1, vmin=-1)

We observe that loc_ic_mou_6, std_ic_mou_6 and total_ic_mou_6 have strong correlation with other features. These features should be taken care of to handle multicollinearity.

[ ]
  1
high_df.drop(['loc_ic_mou_6', 'std_ic_mou_6','total_ic_mou_6'],1,inplace=True)
k) Total incoming calls made in the month of july

[ ]
  1
  2
  3
ic_col_7 = ['loc_ic_mou_7','std_ic_mou_7','isd_ic_mou_7','total_ic_mou_7','std_ic_t2t_mou_7','std_ic_t2m_mou_7','std_ic_t2f_mou_7','loc_ic_t2t_mou_7','loc_ic_t2m_mou_7','loc_ic_t2f_mou_7']
plt.figure(figsize=[8,6])
sns.heatmap(high_df[ic_col_7].corr(), annot=True,cmap="BrBG" , robust=True,linewidth=0.1, vmin=-1)

We observe that loc_ic_mou_7, std_ic_mou_7 and total_ic_mou_7 have strong correlation with other features. These features should be taken care of to handle multicollinearity.

[ ]
  1
high_df.drop(['loc_ic_mou_7', 'std_ic_mou_7','total_ic_mou_7'],1,inplace=True)
l) Total incoming calls made in the month of August

[ ]
  1
  2
  3
ic_col_8 = ['loc_ic_mou_8','std_ic_mou_8','isd_ic_mou_8','total_ic_mou_8','std_ic_t2t_mou_8','std_ic_t2m_mou_8','std_ic_t2f_mou_8','loc_ic_t2t_mou_8','loc_ic_t2m_mou_8','loc_ic_t2f_mou_8']
plt.figure(figsize=[8,6])
sns.heatmap(high_df[ic_col_8].corr(), annot=True,cmap="BrBG" , robust=True,linewidth=0.1, vmin=-1)

We observe that loc_ic_mou_8, std_ic_mou_8 and total_ic_mou_8 have strong correlation with other features. These features should be taken care of to handle multicollinearity.

[ ]
  1
high_df.drop(['loc_ic_mou_8', 'std_ic_mou_8','total_ic_mou_8'],1,inplace=True)
[ ]
  1
  2
# Let us also drop age_group from our dataset as aon_year & age_group are highly correlated.
high_df.drop('age_group',1,inplace=True)
[ ]
  1
  2
  3
# The shape of final dataset

print("The shape of final dataset is :",high_df.shape)
The shape of final dataset is : (30001, 107)
[ ]
  1
high_df.head()

b) Encoding categorical variables
[ ]
  1
  2
  3
  4
  5
# encode categorical variables using Label Encoder

# select all categorical variables
df_categorical = high_df.select_dtypes(include=['object'])
df_categorical.drop('churn',1).head()

[ ]
  1
  2
  3
  4
  5
from sklearn import preprocessing
# apply Label encoder to df_categorical
le = preprocessing.LabelEncoder()
df_categorical = df_categorical.apply(le.fit_transform)
df_categorical.head()

[ ]
  1
  2
  3
  4
  5
# concat df_categorical with original df
high_df = high_df.drop(df_categorical.columns, axis=1)

high_df = pd.concat([high_df, df_categorical], axis=1)
high_df.head()

3) Model building
The main objective behind building these models is identifying important predictor attributes which help the business understand indicators of churn.

MODEL 1 - Tree Model Regression
[ ]
  1
high_tree = high_df.copy()
[ ]
  1
  2
  3
  4
  5
# Putting response variable to y
y = high_tree.pop('churn')

# Putting feature variables to X
X = high_tree
[ ]
  1
  2
print("Size of Feature variables dataset is:",X.shape)
print("Size of response variable dataset is:",y.shape)
Size of Feature variables dataset is: (30001, 106)
Size of response variable dataset is: (30001,)
[ ]
  1
  2
  3
# splitting the data into train & test
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size =0.3, random_state=100, stratify = y)
X_train.head()

Decision tree with default parameters

[ ]
  1
  2
  3
  4
  5
  6
  7
# Importing decision tree classifier from sklearn library
from sklearn.tree import DecisionTreeClassifier

# Fitting the decision tree with default hyperparameters, apart from
# max_depth which is 5 so that we can plot and read the tree.
dt_default = DecisionTreeClassifier(random_state = 100, class_weight='balanced')
dt_default.fit(X_train, y_train)
DecisionTreeClassifier(ccp_alpha=0.0, class_weight='balanced', criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_impurity_split=None,
                       min_samples_leaf=1, min_samples_split=2,
                       min_weight_fraction_leaf=0.0, presort='deprecated',
                       random_state=100, splitter='best')
[ ]
  1
  2
  3
  4
  5
  6
  7
# Let's check the evaluation metrics of our default model

# Making predictions
y_pred_default = dt_default.predict(X_test)

# Printing classification report
print(classification_report(y_test, y_pred_default))
              precision    recall  f1-score   support

           0       0.95      0.95      0.95      8269
           1       0.48      0.48      0.48       732

    accuracy                           0.92      9001
   macro avg       0.72      0.71      0.72      9001
weighted avg       0.91      0.92      0.92      9001

[ ]
  1
  2
  3
  4
  5
  6
  7
  8
  9
 10
 11
 12
 13
 14
 15
 16
 17
 18
 19
 20
 21
import sklearn.metrics as metrics 

# Plotting AUC curve
logit_roc_auc = metrics.roc_auc_score(y_test, y_pred_default)
fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_default, drop_intermediate =True)
plt.figure(figsize=(8,8))
plt.plot(fpr, tpr, label='DTree Default(area = %0.2f)' % logit_roc_auc)
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('FPR -False Positive Rate')
plt.ylabel('TPR - True Positive Rate')
plt.title('Receiver operating characteristic (ROC curve)')
plt.legend(loc="lower right")
plt.grid(True)
plt.show()

print("Area under curve is:", round(metrics.roc_auc_score(y_test, y_pred_default),2))
print("Recall for our model is:" , round(metrics.recall_score(y_test, y_pred_default),2))
print("Accuracy on test set is:" , round(metrics.accuracy_score(y_test, y_pred_default),2))
print("Confusion matrix for our model is:" , metrics.confusion_matrix(y_test, y_pred_default))

Hyperparameter Tuning

The results of default tree are quite poor, and we need to improve it by tuning the hyperparameters.

Tuning max_depth

[ ]
  1
  2
  3
  4
  5
  6
  7
  8
  9
 10
 11
 12
 13
 14
 15
 16

# specify number of folds for k-fold CV
n_folds = 5

# parameters to build the model on
parameters = {'max_depth': range(1, 10,1)}

# instantiate the model
dtree = DecisionTreeClassifier(random_state = 100, class_weight='balanced')

# fit tree on training data
tree = GridSearchCV(dtree, parameters,cv=n_folds,scoring="recall",return_train_score=True, n_jobs=-1, 
                    error_score = 'raise',verbose = 1)
tree.fit(X_train, y_train)
print(tree.best_score_)
print(tree.best_params_)
Fitting 5 folds for each of 9 candidates, totalling 45 fits
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.
[Parallel(n_jobs=-1)]: Done  45 out of  45 | elapsed:   19.2s finished
0.8104199893673577
{'max_depth': 5}
[ ]
  1
  2
  3
  4
  5
# scores of GridSearch CV
scores = tree.cv_results_
pd.DataFrame(scores).head()



[ ]
  1
  2
  3
  4
  5
  6
  7
  8
  9
 10
 11
 12
 13
 14
# plotting accuracies with max_depth
plt.figure(figsize=(8,8))
plt.plot(scores["param_max_depth"], 
         scores["mean_train_score"], 
         label="training recall")
plt.plot(scores["param_max_depth"], 
         scores["mean_test_score"], 
         label="test recall")
plt.xlabel("max_depth")
plt.ylabel("Recall")
plt.legend()
plt.grid(True)
plt.show()


The model perfroms the best at max_depth = 5 and after that the increase in depth drops the accuracy on the test dataset.

Tuning min_samples_leaf

[ ]
  1
  2
  3
  4
  5
  6
  7
  8
  9
 10
 11
 12
 13
 14
 15
 16
# specify number of folds for k-fold CV
n_folds = 5

# parameters to build the model on
parameters = {'min_samples_leaf': range(5, 200, 5)}

# instantiate the model
dtree = DecisionTreeClassifier(random_state = 100, class_weight='balanced')

# fit tree on training data
tree = GridSearchCV(dtree, parameters,cv=n_folds, 
                   scoring="recall", n_jobs = -1,return_train_score=True, verbose = 1)
tree.fit(X_train, y_train)
print(tree.best_score_)
print(tree.best_params_)

Fitting 5 folds for each of 39 candidates, totalling 195 fits
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.
[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   27.6s
[Parallel(n_jobs=-1)]: Done 195 out of 195 | elapsed:  1.6min finished
0.8250295827545402
{'min_samples_leaf': 120}
[ ]
  1
  2
  3
scores = tree.cv_results_
pd.DataFrame(scores).head()


[ ]
  1
  2
  3
  4
  5
  6
  7
  8
  9
 10
 11
 12
 13
# plotting accuracies with min_samples_leaf
plt.figure(figsize = (8,8))
plt.plot(scores["param_min_samples_leaf"], 
         scores["mean_train_score"], 
         label="training recall")
plt.plot(scores["param_min_samples_leaf"], 
         scores["mean_test_score"], 
         label="test recall")
plt.xlabel("min_samples_leaf")
plt.ylabel("Recall")
plt.legend()
plt.grid(True)
plt.show()

As seen from the above plot, accuracy of the model increases as min_samples_leaf increases. A steep increase from 0 to 115 and it kind of flattens rest of the way.

Tuning min_samples_split

[ ]
  1
  2
  3
  4
  5
  6
  7
  8
  9
 10
 11
 12
 13
 14
 15
 16
# specify number of folds for k-fold CV
n_folds = 5

# parameters to build the model on
parameters = {'min_samples_split': range(300,450,5)}

# instantiate the model
dtree = DecisionTreeClassifier(random_state = 100, class_weight='balanced')

# fit tree on training data
tree = GridSearchCV(dtree, parameters, return_train_score=True,
                    cv=n_folds, verbose=1,
                   scoring="recall",n_jobs=-1)
tree.fit(X_train, y_train)
print(tree.best_score_)
print(tree.best_params_)
Fitting 5 folds for each of 30 candidates, totalling 150 fits
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.
[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   30.7s
[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:  1.8min finished
0.8191936341342114
{'min_samples_split': 395}
[ ]
  1
  2
  3
# scores of GridSearch CV
scores = tree.cv_results_
pd.DataFrame(scores).head()

[ ]
  1
  2
  3
  4
  5
  6
  7
  8
  9
 10
 11
 12
 13
 14
# plotting accuracies with min_samples_leaf
plt.figure()
plt.plot(scores["param_min_samples_split"], 
         scores["mean_train_score"], 
         label="training recall")
plt.plot(scores["param_min_samples_split"], 
         scores["mean_test_score"], 
         label="test recall")
plt.xlabel("min_samples_split")
plt.ylabel("Recall")
plt.legend()
plt.grid(True)
plt.show()


The test accuracy of the model increases as the min_samples_split increases.

Grid Search to Find Optimal Hyperparameters
We can now use GridSearchCV to find multiple optimal hyperparameters together. Note that this time, we'll also specify the criterion (gini/entropy or IG).

[ ]
  1
  2
  3
  4
  5
  6
  7
  8
  9
 10
 11
 12
 13
 14
 15
 16
 17
 18
# Create the parameter grid 
param_grid = {
    'max_depth': range(3,10, 2),
    'min_samples_leaf': range(100, 125, 5),
    'min_samples_split': range(380,400,5),
    'criterion': ["entropy", "gini"]
}

n_folds = 5

# Instantiate the grid search model
dtree = DecisionTreeClassifier(random_state = 100, class_weight='balanced')
grid_search = GridSearchCV(estimator = dtree, param_grid = param_grid, 
                          cv = n_folds, verbose = 1, n_jobs=-1)

# Fit the grid search to the data
grid_search.fit(X_train,y_train)

Fitting 5 folds for each of 160 candidates, totalling 800 fits
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.
[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   16.9s
[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  1.3min
[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  3.2min
[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:  5.5min
[Parallel(n_jobs=-1)]: Done 800 out of 800 | elapsed:  5.6min finished
GridSearchCV(cv=5, error_score=nan,
             estimator=DecisionTreeClassifier(ccp_alpha=0.0,
                                              class_weight='balanced',
                                              criterion='gini', max_depth=None,
                                              max_features=None,
                                              max_leaf_nodes=None,
                                              min_impurity_decrease=0.0,
                                              min_impurity_split=None,
                                              min_samples_leaf=1,
                                              min_samples_split=2,
                                              min_weight_fraction_leaf=0.0,
                                              presort='deprecated',
                                              random_state=100,
                                              splitter='best'),
             iid='deprecated', n_jobs=-1,
             param_grid={'criterion': ['entropy', 'gini'],
                         'max_depth': range(3, 10, 2),
                         'min_samples_leaf': range(100, 125, 5),
                         'min_samples_split': range(380, 400, 5)},
             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,
             scoring=None, verbose=1)
[ ]
  1
  2
print(grid_search.best_score_)
print(grid_search.best_params_)
0.8519523809523809
{'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 100, 'min_samples_split': 380}
[ ]
  1
  2
  3
# cv results
cv_results = pd.DataFrame(grid_search.cv_results_)
cv_results

Running the model with best parameters obtained from grid search.

[ ]
  1
  2
  3
  4
  5
  6
  7
# model with optimal hyperparameters
clf_gini = DecisionTreeClassifier(criterion = "entropy", 
                                  random_state = 100,
                                  max_depth=grid_search.best_params_['max_depth'],
                                  min_samples_leaf=grid_search.best_params_['min_samples_leaf'],
                                  min_samples_split=grid_search.best_params_['min_samples_split'], class_weight='balanced')
clf_gini.fit(X_train, y_train)
DecisionTreeClassifier(ccp_alpha=0.0, class_weight='balanced',
                       criterion='entropy', max_depth=5, max_features=None,
                       max_leaf_nodes=None, min_impurity_decrease=0.0,
                       min_impurity_split=None, min_samples_leaf=100,
                       min_samples_split=380, min_weight_fraction_leaf=0.0,
                       presort='deprecated', random_state=100, splitter='best')
[ ]
  1
  2
# accuracy score
clf_gini.score(X_train,y_train)
0.8254285714285714
The accuracy of the Decision Tree model on the train dataset is around 82%, which is really good for a model to be classified as a good model.

[ ]
  1
  2
  3
  4
  5
  6
  7
  8
  9
 10
 11
 12
 13
 14
 15
 16
 17
 18
 19
 20
 21
# classification metrics

y_pred = clf_gini.predict(X_test)

logit_roc_auc = metrics.roc_auc_score(y_test, y_pred)
fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)
plt.figure()
plt.plot(fpr, tpr, label='DTree Optimised(area = %0.2f)' % logit_roc_auc)
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic')
plt.legend(loc="lower right")
plt.show()

print("Area under curve is:", round(metrics.roc_auc_score(y_test, y_pred),2))
print("Recall for our model is:" , round(metrics.recall_score(y_test, y_pred),2))
print("Accuracy on test set is:" , round(metrics.accuracy_score(y_test, y_pred),2))
print("Confusion matrix for our model is:" , metrics.confusion_matrix(y_test, y_pred))

[ ]
  1
  2
  3
  4
  5
# accuracy score on training set

y_train_pred = clf_gini.predict(X_train)
y_train_pred = y_train_pred.reshape(-1,1)
print("Accuracy on train set is:" , round(metrics.accuracy_score(y_test, y_pred),2))
Accuracy on train set is: 0.82
[ ]
  1
print(classification_report(y_test, y_pred))
              precision    recall  f1-score   support

           0       0.98      0.81      0.89      8269
           1       0.28      0.84      0.42       732

    accuracy                           0.82      9001
   macro avg       0.63      0.83      0.66      9001
weighted avg       0.93      0.82      0.85      9001

[ ]
  1
  2
  3
# confusion matrix
cm = confusion_matrix(y_test,y_pred)
print(cm)
[[6724 1545]
 [ 118  614]]
[ ]
  1
  2
  3
  4
  5
speci = round((cm[0,0]/(cm[0,0]+cm[0,1])),2)
sensi = round((cm[1,1]/(cm[1,0]+cm[1,1])),2)

print('The specificity is:',speci)
print('The sensitivity is:',sensi)
The specificity is: 0.81
The sensitivity is: 0.84
INFERENCE

The accuracy of the Decision Tree model on the TEST dataset is 83% and train set is 82% , which is pretty good.
The RECALL score for both Churn and Non-Churns is around 80%.
specificity obtained is : 81%
sensitivity obtained is: 84%
From the above metrics, we can conclude that the above is a decent one.

MODEL 2 - Random Forest
[ ]
  1
high_rf = high_df.copy()
[ ]
  1
  2
  3
  4
  5
# Putting response variable to y
y = high_rf.pop('churn')

# Putting feature variables to X
X = high_rf
[ ]
  1
  2
print("Size of Feature variables dataset is:",X.shape)
print("Size of response variable dataset is:",y.shape)
Size of Feature variables dataset is: (30001, 106)
Size of response variable dataset is: (30001,)
[ ]
  1
  2
  3
# splitting the data into train & test
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size =0.3, random_state=100, stratify = y)
X_train.head()

Default Hyperparameters
Let's first fit a random forest model with default hyperparameters.

[ ]
  1
  2
  3
  4
  5
  6
  7
  8
  9
# Importing random forest classifier from sklearn library
from sklearn.ensemble import RandomForestClassifier

# Running the random forest with default parameters
rfc = RandomForestClassifier(class_weight = 'balanced',random_state=100,n_jobs = -1)

# fit
rfc.fit(X_train,y_train)

RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_impurity_split=None,
                       min_samples_leaf=1, min_samples_split=2,
                       min_weight_fraction_leaf=0.0, n_estimators=100,
                       n_jobs=-1, oob_score=False, random_state=100, verbose=0,
                       warm_start=False)
[ ]
  1
  2
  3
  4
  5
  6
  7
  8
  9
 10
 11
 12
 13
 14
 15
 16
 17
 18
 19
 20
 21
# Making Predictions
y_pred_default = rfc.predict(X_test)


logit_roc_auc = metrics.roc_auc_score(y_test, y_pred_default )
fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_default )
plt.figure()
plt.plot(fpr, tpr, label='RFC Default (area = %0.2f)' % logit_roc_auc)
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic')
plt.legend(loc="lower right")
plt.show()

print("Area under curve is:", round(metrics.roc_auc_score(y_test, y_pred_default),2))
print("Recall for our model is:" , round(metrics.recall_score(y_test, y_pred_default),2))
print("Accuracy on test set is:" , round(metrics.accuracy_score(y_test, y_pred_default),2))
print("Confusion matrix for our model is:" , metrics.confusion_matrix(y_test, y_pred_default))

[ ]
  1
  2
  3
  4
  5
# Importing classification report and confusion matrix from sklearn metrics
from sklearn.metrics import classification_report,confusion_matrix, accuracy_score

# Let's check the report of our default model
print(classification_report(y_test,y_pred_default))
              precision    recall  f1-score   support

           0       0.95      0.99      0.97      8269
           1       0.74      0.43      0.55       732

    accuracy                           0.94      9001
   macro avg       0.85      0.71      0.76      9001
weighted avg       0.93      0.94      0.93      9001

[ ]
  1
  2
  3
# Printing confusion matrix
cm = confusion_matrix(y_test,y_pred_default)
print(cm)
[[8158  111]
 [ 414  318]]
[ ]
  1
  2
  3
  4
  5
speci = round((cm[0,0]/(cm[0,0]+cm[0,1])),2)
sensi = round((cm[1,1]/(cm[1,0]+cm[1,1])),2)

print('The specificity is:',speci)
print('The sensitivity is:',sensi)
The specificity is: 0.99
The sensitivity is: 0.43
[ ]
  1
print(accuracy_score(y_test,y_pred_default))
0.9416731474280635
The model does not do good. let's now look at the list of hyperparameters which we can tune to improve model performance.

Hyperparameter Tuning

The results of default tree are poor, and we need to improve it by tuning the hyperparameters.

Tuning max_depth

[ ]
  1
  2
  3
  4
  5
  6
  7
  8
  9
 10
 11
 12
 13
 14
 15
 16
# parameters to build the model on
parameters = {'max_depth': range(2, 15, 1)}

# number of folds
n_folds = 5

# instantiate the model
rf = RandomForestClassifier(class_weight = 'balanced',random_state=0)


# fit tree on training data
rf = GridSearchCV(rf, parameters, return_train_score=True,cv=n_folds, n_jobs=-1,
                   scoring="recall",verbose = 1)
rf.fit(X_train, y_train)
print(rf.best_score_)
print(rf.best_params_)
Fitting 5 folds for each of 13 candidates, totalling 65 fits
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.
[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  2.4min
[Parallel(n_jobs=-1)]: Done  65 out of  65 | elapsed:  4.0min finished
0.7577335322666393
{'max_depth': 5}
[ ]
  1
  2
  3
# scores of GridSearch CV
scores = rf.cv_results_
pd.DataFrame(scores).head()

[ ]
  1
  2
  3
  4
  5
  6
  7
  8
  9
 10
 11
 12
 13
 14
# plotting accuracies with max_depth
plt.figure(figsize=(8,8))
plt.plot(scores["param_max_depth"], 
         scores["mean_train_score"], 
         label="training recall")
plt.plot(scores["param_max_depth"], 
         scores["mean_test_score"], 
         label="test recall")
plt.xlabel("max_depth")
plt.ylabel("Recall")
plt.grid(True)
plt.legend()
plt.show()


We observe that with increase in the value of max_depth, both train and test scores increase till a point, but after that test score starts to decrease. The ensemble tries to overfit as we increase the max_depth. The model perfroms the best at max_depth = 5 and after that the increase in depth drops the accuracy on the test dataset.

Tuning n_estimators

Let's try to find the optimum values for n_estimators and understand how the value of n_estimators impacts the overall accuracy. We'll specify an appropriately low value of max_depth, so that the trees do not overfit.

[ ]
  1
  2
  3
  4
  5
  6
  7
  8
  9
 10
 11
 12
 13
 14
 15
 16
# specify number of folds for k-fold CV
n_folds = 5

# parameters to build the model on
parameters = {'n_estimators': [10,25,50,75,100]}

# instantiate the model (note we are specifying a max_depth)
rf = RandomForestClassifier(random_state = 100,class_weight='balanced')


# fit tree on training data
rf = GridSearchCV(rf, parameters,cv=n_folds, verbose=1,scoring="recall",return_train_score = True, 
                  n_jobs=-1)
rf.fit(X_train, y_train)
print(rf.best_score_)
print(rf.best_params_)
Fitting 5 folds for each of 5 candidates, totalling 25 fits
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.
[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:  1.3min finished
0.43767042239028653
{'n_estimators': 75}
[ ]
  1
  2
  3
# scores of GridSearch CV
scores = rf.cv_results_
pd.DataFrame(scores).head()

[ ]
  1
  2
  3
  4
  5
  6
  7
  8
  9
 10
 11
 12
 13
 14
# plotting accuracies with n_estimators
plt.figure(figsize=(8,8))
plt.plot(scores["param_n_estimators"], 
         scores["mean_train_score"], 
         label="training recall")
plt.plot(scores["param_n_estimators"], 
         scores["mean_test_score"], 
         label="test recall")
plt.xlabel("n_estimators")
plt.ylabel("Recall")
plt.legend()
plt.grid(True)
plt.show()


Tuning min_samples_split

Let's now look at the performance of the ensemble as we vary min_samples_split.

[ ]
  1
  2
  3
  4
  5
  6
  7
  8
  9
 10
 11
 12
 13
# specify number of folds for k-fold CV
n_folds = 3

# parameters to build the model on
parameters = {"min_samples_split": range(100,200,10)}

# instantiate the model (note we are specifying a max_depth)
rf = RandomForestClassifier(class_weight = 'balanced',random_state=0)

# fit tree on training data
rf = GridSearchCV(rf, parameters, n_jobs = -1,cv=n_folds, verbose = 1,
                   scoring="recall",return_train_score=True)
rf.fit(X_train, y_train)
Fitting 3 folds for each of 10 candidates, totalling 30 fits
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.
[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:  1.9min finished
GridSearchCV(cv=3, error_score=nan,
             estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                                              class_weight='balanced',
                                              criterion='gini', max_depth=None,
                                              max_features='auto',
                                              max_leaf_nodes=None,
                                              max_samples=None,
                                              min_impurity_decrease=0.0,
                                              min_impurity_split=None,
                                              min_samples_leaf=1,
                                              min_samples_split=2,
                                              min_weight_fraction_leaf=0.0,
                                              n_estimators=100, n_jobs=None,
                                              oob_score=False, random_state=0,
                                              verbose=0, warm_start=False),
             iid='deprecated', n_jobs=-1,
             param_grid={'min_samples_split': range(100, 200, 10)},
             pre_dispatch='2*n_jobs', refit=True, return_train_score=True,
             scoring='recall', verbose=1)
[ ]
  1
  2
print(rf.best_score_)
print(rf.best_params_)
0.7753039599584786
{'min_samples_split': 180}
[ ]
  1
  2
  3
# scores of GridSearch CV
scores = rf.cv_results_
pd.DataFrame(scores).head()

[ ]
  1
  2
  3
  4
  5
  6
  7
  8
  9
 10
 11
 12
 13
# plotting accuracies with min_samples_split
plt.figure(figsize=(8,8))
plt.plot(scores["param_min_samples_split"], 
         scores["mean_train_score"], 
         label="training recall")
plt.plot(scores["param_min_samples_split"], 
         scores["mean_test_score"], 
         label="test recall")
plt.xlabel("min_samples_split")
plt.ylabel("Recall")
plt.legend()
plt.grid(True)
plt.show()

Tuning min_samples_leaf

[ ]
  1
  2
  3
  4
  5
  6
  7
  8
  9
 10
 11
 12
 13
# specify number of folds for k-fold CV
n_folds = 3

# parameters to build the model on
parameters = {"min_samples_leaf": range(1,200,20)}

# instantiate the model (note we are specifying a max_depth)
rf = RandomForestClassifier(class_weight = 'balanced',random_state=0)

# fit tree on training data
rf = GridSearchCV(rf, parameters, n_jobs = -1,cv=n_folds, verbose = 1,
                   scoring="recall",return_train_score=True)
rf.fit(X_train, y_train)
Fitting 3 folds for each of 10 candidates, totalling 30 fits
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.
[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:  1.8min finished
GridSearchCV(cv=3, error_score=nan,
             estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                                              class_weight='balanced',
                                              criterion='gini', max_depth=None,
                                              max_features='auto',
                                              max_leaf_nodes=None,
                                              max_samples=None,
                                              min_impurity_decrease=0.0,
                                              min_impurity_split=None,
                                              min_samples_leaf=1,
                                              min_samples_split=2,
                                              min_weight_fraction_leaf=0.0,
                                              n_estimators=100, n_jobs=None,
                                              oob_score=False, random_state=0,
                                              verbose=0, warm_start=False),
             iid='deprecated', n_jobs=-1,
             param_grid={'min_samples_leaf': range(1, 200, 20)},
             pre_dispatch='2*n_jobs', refit=True, return_train_score=True,
             scoring='recall', verbose=1)
[ ]
  1
  2
print(rf.best_score_)
print(rf.best_params_)
0.7922753574034677
{'min_samples_leaf': 181}
[ ]
  1
  2
  3
# scores of GridSearch CV
scores = rf.cv_results_
pd.DataFrame(scores).head()

[ ]
  1
  2
  3
  4
  5
  6
  7
  8
  9
 10
 11
 12
 13
# plotting accuracies with min_samples_split
plt.figure(figsize=(8,8))
plt.plot(scores["param_min_samples_leaf"], 
         scores["mean_train_score"], 
         label="training recall")
plt.plot(scores["param_min_samples_leaf"], 
         scores["mean_test_score"], 
         label="test recall")
plt.xlabel("min_samples_leaf")
plt.ylabel("Recall")
plt.legend()
plt.grid(True)
plt.show()

As seen from the above plot, recall of the model increases as min_samples_leaf increases. A steep increase is observed from 0 to 25. The curve flattens after min_samples_leaf = 125. So let us choose this as our Optimal minimum number of samples leaf.

Tuning max_features

Let's see how the model performance varies with max_features, which is the maximum number of features considered for splitting at a node.

[ ]
  1
  2
  3
  4
  5
  6
  7
  8
  9
 10
 11
 12
 13
# specify number of folds for k-fold CV
n_folds = 3

# parameters to build the model on
parameters = {'max_features': [25,50,75]}

# instantiate the model (note we are specifying a max_depth)
rf = RandomForestClassifier(max_depth=4, class_weight = 'balanced',random_state=100)

# fit tree on training data
rf = GridSearchCV(rf, parameters, n_jobs  = -1,cv=n_folds, verbose = 1,
                   scoring="recall",return_train_score=True)
rf.fit(X_train, y_train)
Fitting 3 folds for each of 3 candidates, totalling 9 fits
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.
[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed:   60.0s finished
GridSearchCV(cv=3, error_score=nan,
             estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                                              class_weight='balanced',
                                              criterion='gini', max_depth=4,
                                              max_features='auto',
                                              max_leaf_nodes=None,
                                              max_samples=None,
                                              min_impurity_decrease=0.0,
                                              min_impurity_split=None,
                                              min_samples_leaf=1,
                                              min_samples_split=2,
                                              min_weight_fraction_leaf=0.0,
                                              n_estimators=100, n_jobs=None,
                                              oob_score=False, random_state=100,
                                              verbose=0, warm_start=False),
             iid='deprecated', n_jobs=-1,
             param_grid={'max_features': [25, 50, 75]}, pre_dispatch='2*n_jobs',
             refit=True, return_train_score=True, scoring='recall', verbose=1)
[ ]
  1
  2
print(rf.best_score_)
print(rf.best_params_)
0.7975374875384125
{'max_features': 75}
[ ]
  1
  2
  3
# scores of GridSearch CV
scores = rf.cv_results_
pd.DataFrame(scores).head()

[ ]
  1
  2
  3
  4
  5
  6
  7
  8
  9
 10
 11
 12
 13
# plotting accuracies with max_features
plt.figure()
plt.plot(scores["param_max_features"], 
         scores["mean_train_score"], 
         label="training recall")
plt.plot(scores["param_max_features"], 
         scores["mean_test_score"], 
         label="test recall")
plt.xlabel("max_features")
plt.ylabel("Recall")
plt.legend()
plt.show()


Apparently, the training and test scores both seem to increase as we increase max_features, and the model doesn't seem to overfit more with increasing max_features.

Grid Search to Find Optimal Hyperparameters
We can now find the optimal hyperparameters using GridSearchCV.

[ ]
  1
  2
  3
  4
  5
  6
  7
  8
  9
 10
 11
 12
 13
 14
 15
 16
# Create the parameter grid based on the results of random search 
param_grid = {
    'max_depth': [8,8,10],
    'min_samples_split': [170,190,200], 'min_samples_leaf': [120,130,140] }

# Create a based model
rf = RandomForestClassifier(class_weight = 'balanced',random_state=100)

# Instantiate the grid search model
grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, 
                          cv = 3, n_jobs = -1,verbose = 1,return_train_score=True)

# Fit the grid search to the data
grid_search.fit(X_train, y_train)
print(grid_search.best_score_)
print(grid_search.best_params_)
Fitting 3 folds for each of 27 candidates, totalling 81 fits
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.
[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  2.4min
[Parallel(n_jobs=-1)]: Done  81 out of  81 | elapsed:  4.0min finished
0.8882857142857142
{'max_depth': 8, 'min_samples_leaf': 120, 'min_samples_split': 170}
[ ]
  1
  2
# printing the optimal accuracy score and hyperparameters
print('We can get accuracy of',grid_search.best_score_,'using',grid_search.best_params_)
We can get accuracy of 0.8882857142857142 using {'max_depth': 8, 'min_samples_leaf': 120, 'min_samples_split': 170}
Fitting the final model with the best parameters obtained from grid search.

[ ]
  1
  2
  3
  4
  5
  6
# model with the best hyperparameters
from sklearn.ensemble import RandomForestClassifier
rfc = RandomForestClassifier(bootstrap=True,random_state=100,
                             max_depth=grid_search.best_params_['max_depth'],
                             min_samples_split=grid_search.best_params_['min_samples_split'],
                              class_weight='balanced')
[ ]
  1
  2
# fit
rfc.fit(X_train,y_train)
RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',
                       criterion='gini', max_depth=8, max_features='auto',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_impurity_split=None,
                       min_samples_leaf=1, min_samples_split=170,
                       min_weight_fraction_leaf=0.0, n_estimators=100,
                       n_jobs=None, oob_score=False, random_state=100,
                       verbose=0, warm_start=False)
[ ]
123456789101112131415161718192021
# predict
predictions = rfc.predict(X_test)

# Plotting ROC
logit_roc_auc = metrics.roc_auc_score(y_test, predictions)
fpr, tpr, thresholds = metrics.roc_curve(y_test, predictions)
plt.figure()
plt.plot(fpr, tpr, label='RFC optimised (area = %0.2f)' % logit_roc_auc)
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0.0, 1.0])


[ ]
  1
  2
  3
  4
  5
# accuracy score on training set

y_train_pred = rfc.predict(X_train)
y_train_pred = y_train_pred.reshape(-1,1)
print("Accuracy on train set is:" , round(metrics.accuracy_score(y_test, y_pred),2))
Accuracy on train set is: 0.82
[ ]
  1
print(classification_report(y_test,predictions))
              precision    recall  f1-score   support

           0       0.98      0.91      0.94      8269
           1       0.44      0.78      0.56       732

    accuracy                           0.90      9001
   macro avg       0.71      0.85      0.75      9001
weighted avg       0.94      0.90      0.91      9001

[ ]
  1
  2
  3
# confusion matrix
cm = confusion_matrix(y_test,predictions)
print(cm)
[[7543  726]
 [ 162  570]]
[ ]
  1
  2
  3
  4
  5
speci = cm[0,0]/(cm[0,0]+cm[0,1])
sensi = cm[1,1]/(cm[1,0]+cm[1,1])

print('The specificity is:',speci)
print('The sensitivity is:',sensi)
The specificity is: 0.9122022009916556
The sensitivity is: 0.7786885245901639
INFERENCE

The accuracy of the Random Forest Tree model on the TEST dataset is 85% % training dataset is 82%, which is pretty good for a model.
The RECALL score for Churn is 78% and Non-Churns is around 91%.
specificity obtained is : 91%
sensitivity obtained is: 78%
From the above metrics, we can conclude that the above model performs well.

3) MODEL SUMMARY
In our case study we have chosen Recall as the Performace measure because it is more important to predict the customers who are more likely to churn rather than accuracy.

[ ]
  1
  2
  3
  4
  5
  6
  7
  8
  9
model_stats = pd.DataFrame(
    {'model':['DTree - Default Param','DTree - Tuned Param','RFC - Default Param','RFC - Tuned Param'],
                      
    'Recall':       [0.48,0.84,0.43,0.78],
    'Test accuracy':     [0.92,0.82,0.94,0.85],
    'Roc_auc_score':[0.71,0.83,0.71,0.85],
    })

model_stats.sort_values(by=['Recall','Test accuracy','Roc_auc_score'],ascending=False)

Decision tree with tuned hyperparameters outperforms all the other models in terms of recall and has a pretty decent accuracy and AUC score. Let us choose this model to find out the most important features affecting churn.

[ ]
  1
  2
  3
  4
features = clf_gini.feature_importances_
cols = X.iloc[:,sorted(range(len(features)), key=lambda i: features[i])[-10:]].columns
cols

Index(['arpu_7', 'max_rech_amt_6', 'std_og_t2m_mou_8', 'loc_og_t2m_mou_8',
       'max_rech_data_8', 'last_day_rch_amt_8', 'total_data_rech_8',
       'total_amt_8', 'roam_og_mou_8', 'loc_ic_t2m_mou_8'],
      dtype='object')
[ ]
  1
  2
  3
  4
  5
  6
  7
  8
# plotting the tree
dot_data = StringIO()  
features = X.columns
export_graphviz(clf_gini, out_file=dot_data,
                feature_names=features, filled=True,rounded=True)

graph = pydot.graph_from_dot_data(dot_data.getvalue())  
Image(graph[0].create_png())

4) Strategies to be incorporated:
In the telecom industry, customers are able to choose from multiple service providers and actively switch from one operator to another. In this highly competitive market, the telecommunications industry experiences an average of 15-25% annual churn rate.

Given the fact that it costs 5-10 times more to acquire a new customer than to retain an existing one, customer retention has now become even more important than customer acquisition.

In order to manage High Value Customer Churn, we have predicted customers that are more likely to churn and the factors that influence the high churn.

From the exploratory analysis, we observed that there is considerable drop in recharge, call usage and data usage in the 8th month which is the Action Phase. From the list of important predictors affecting the churn, this is again evident as follows:

'arpu_7',
'max_rech_amt_6',
'std_og_t2m_mou_8',
'loc_og_t2m_mou_8',
'max_rech_data_8',
'last_day_rch_amt_8',
'total_data_rech_8',
'total_amt_8',
'roam_og_mou_8',
'loc_ic_t2m_mou_8'
Average revenue per user in the 7th month plays a vital role in deciding churn. A sudden drop in it might indicate that the customer might be thinking about churning and appropriate actions should be taken.

Local & STD Minutes of usage (incoming & outgoing) are the most affecting features on the customer churn.

Lastday of recharge amount in the action phase also plays a crucial role in determing churn.

The maximum rcharge for calling data by a client in the 6th Month and 8th Month should be carefully focussed as the 6th month indicates the begining of the good phase and 8th month indicates the action phase.

Last day of recharge in the 8th month, the total recharge for data done in the 8th month and the total amount spent on calls and data by clients in 8th month also play crucial role in indicating churn.

Outgoing roaming calls made by clients in the 8th month also play key role in indicating churn.

Following strategies can be incorporated :

Sudden drop in Local & STD Minutes of usage might be because of the unsatisfactory customer service or because of poor network or unsuitable customer schemes/plans. Efforts shall be made to provide better network and focus on customer satisfaction.
Customised plans should be provided to such customers to stop them from churning.
Based on the usage / last recharge, routine feedback calls to understand the customer satisfaction regarding services can be made to understand their grievances & expectations. Appropriate action should be taken to avoid them rom churning.
Various attractive offers can be introduced to customers showing sudden drop in total amount spent on calls & data recharge in the action phase to lure them.
Promotional offers can also be very helpful

